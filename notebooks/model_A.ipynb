{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10b72ae",
   "metadata": {},
   "source": [
    "# Model A: Time-Frequency Domain U-Net for Music Source Separation\n",
    "\n",
    "This notebook demonstrates music source separation using a U-Net architecture in the time-frequency domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b5c35",
   "metadata": {},
   "source": [
    "## 1. Imports and Environment Setup\n",
    "\n",
    "- Import required libraries (torch, numpy, matplotlib, etc.)\n",
    "\n",
    "- Set device (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d461cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from models import utils\n",
    "from models import model_A as ma\n",
    "\n",
    "# Setup Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36209915",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "- Load example mixture and target data (from .npy files)\n",
    "\n",
    "- Visualize waveforms and spectrograms\n",
    "\n",
    "- Normalize or preprocess as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plots to appear inside the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import musdb\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory\n",
    "sys.path.append('..') \n",
    "from models import utils\n",
    "from models.utils import AudioProcessor\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. PREPARE CURRICULUM CACHE\n",
    "# ==============================================================================\n",
    "print(\"â³ Checking Data Cache...\")\n",
    "mus = musdb.DB(download=True)\n",
    "utils.prepare_curriculum_cache(mus, cache_dir=\"../data/curriculum\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. LOAD DATA PATHS FOR BOTH STAGES\n",
    "# ==============================================================================\n",
    "data_root = Path(\"../data/curriculum\")\n",
    "s1_mix_path = data_root / \"stage1\" / \"mixture\"\n",
    "s1_tgt_path = data_root / \"stage1\" / \"target\"\n",
    "s2_mix_path = data_root / \"stage2\" / \"mixture\"\n",
    "s2_tgt_path = data_root / \"stage2\" / \"target\"\n",
    "\n",
    "mix_files_stage1 = sorted(list(s1_mix_path.glob(\"*.npy\")))\n",
    "tgt_files_stage1 = sorted(list(s1_tgt_path.glob(\"*.npy\")))\n",
    "mix_files_stage2 = sorted(list(s2_mix_path.glob(\"*.npy\")))\n",
    "tgt_files_stage2 = sorted(list(s2_tgt_path.glob(\"*.npy\")))\n",
    "\n",
    "print(f\"\\nâœ… Data Ready!\")\n",
    "print(f\"   Stage 1 Samples: {len(mix_files_stage1)}\")\n",
    "print(f\"   Stage 2 Samples: {len(mix_files_stage2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a129c",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "- Show the U-Net model code (import or define)\n",
    "\n",
    "- Print model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617cd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Architecture: Frequency Domain U-Net\n",
    "from models import model_A as ma\n",
    "\n",
    "# Instantiate the U-Net model\n",
    "model = ma.TimeFrequencyDomainUNet(in_channels=1, out_channels=1, base_filters=64, num_layers=4).to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "\n",
    "# Optionally visualize model architecture if utility exists\n",
    "if hasattr(utils, 'visualize_model_architecture'):\n",
    "    utils.visualize_model_architecture(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d10511",
   "metadata": {},
   "source": [
    "## 4. Training Setup\n",
    "\n",
    "- Define loss function and optimizer\n",
    "\n",
    "- Set training hyperparameters (epochs, batch size, learning rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2df9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training Setup\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set training hyperparameters\n",
    "batch_size = 4\n",
    "num_epochs = 20\n",
    "patience = 5\n",
    "\n",
    "print(f\"Loss function: {loss_fn}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Batch size: {batch_size}, Epochs: {num_epochs}, Patience: {patience}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78f629",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23765a5c",
   "metadata": {},
   "source": [
    "### 5.a Overfit Method\n",
    "\n",
    "- Train the model on a very small subset (e.g., one batch or a few samples) to ensure it can overfit and the implementation is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.a Overfit Method: Try to overfit on a tiny subset of Stage 1 data\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Force plots to display inline\n",
    "%matplotlib inline \n",
    "\n",
    "from models import utils\n",
    "from models import model_A as ma\n",
    "\n",
    "# 1. Prepare Data\n",
    "# ----------------------------------------------------------------------------\n",
    "stage1_mix_path = Path(\"../data/curriculum/stage1/mixture\")\n",
    "stage1_tgt_path = Path(\"../data/curriculum/stage1/target\")\n",
    "mix_files = sorted(list(stage1_mix_path.glob(\"*.npy\")))\n",
    "tgt_files = sorted(list(stage1_tgt_path.glob(\"*.npy\")))\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "overfit_idx = 0\n",
    "sr = 22050\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "checkpoint_dir = \"../checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"overfit_checkpoint.pth\")\n",
    "\n",
    "if len(mix_files) > 0 and len(tgt_files) > 0:\n",
    "    # Load raw numpy arrays (2 seconds)\n",
    "    mix_wav = np.load(mix_files[overfit_idx])[:sr*2] \n",
    "    tgt_wav = np.load(tgt_files[overfit_idx])[:sr*2]\n",
    "\n",
    "    # Initialize Processor (needed for Trainer)\n",
    "    processor = utils.AudioProcessor(device=device)\n",
    "\n",
    "    # 2. Create Tiny Dataset (Pass RAW AUDIO, not spectrograms)\n",
    "    class TinyDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, mix, tgt):\n",
    "            self.mix = mix\n",
    "            self.tgt = tgt\n",
    "        def __len__(self):\n",
    "            return 1\n",
    "        def __getitem__(self, idx):\n",
    "            # FIXED: Return keys 'mix' and 'tgt' to match ModelATrainer expectations\n",
    "            return {\n",
    "                'mix': torch.tensor(self.mix, dtype=torch.float32), \n",
    "                'tgt': torch.tensor(self.tgt, dtype=torch.float32)\n",
    "            }\n",
    "\n",
    "    tiny_dataset = TinyDataset(mix_wav, tgt_wav)\n",
    "    tiny_loader = DataLoader(tiny_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # 3. Initialize Model & Trainer\n",
    "    model = ma.TimeFrequencyDomainUNet(in_channels=1, out_channels=1, base_filters=32, num_layers=4).to(device)\n",
    "    \n",
    "    # Large patience effectively disables early stopping for this test\n",
    "    trainer = ma.ModelATrainer(model, tiny_loader, tiny_loader, processor, learning_rate=1e-3, device=device, patience=9999)\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading overfit checkpoint from {checkpoint_path}\")\n",
    "        # Note: If you want to FORCE retraining, delete the checkpoint file manually or uncomment the line below:\n",
    "        # os.remove(checkpoint_path)\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "        print(\"Skipping training (checkpoint loaded).\")\n",
    "    else:\n",
    "        print(\"ðŸš€ Starting Overfit Test...\")\n",
    "        # Capture the history dictionary returned by train()\n",
    "        history = trainer.train(num_epochs=50) # Increased to 50 for better overfit proof\n",
    "        \n",
    "        # Save manually if needed, though trainer usually saves best\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # 4. Plot Loss\n",
    "        if len(history['train_loss']) > 0:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "            plt.plot(history['val_loss'], label='Val Loss', marker='x')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss (L1)')\n",
    "            plt.title('Overfit Training Loss Curve')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ No data found. Please run the caching step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e972d",
   "metadata": {},
   "source": [
    "### 5.b Full Training\n",
    "\n",
    "- Train the model on the full training dataset as intended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.b Full Training: Train on Stage 1, then Stage 2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Force plots to be inline\n",
    "%matplotlib inline \n",
    "\n",
    "from models import utils\n",
    "from models import model_A as ma\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "batch_size = 4\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-4\n",
    "patience = 5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "checkpoint_dir = \"../checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 1. Setup Processor & Model\n",
    "# ------------------------------------------------------------------------------\n",
    "processor = utils.AudioProcessor(device=device)\n",
    "model = ma.TimeFrequencyDomainUNet(in_channels=1, out_channels=1, base_filters=64, num_layers=4).to(device)\n",
    "\n",
    "# ==============================================================================\n",
    "# STAGE 1 TRAINING (Vocals + Other -> Other)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸš€ STAGE 1: Training on Simplified Mixture\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Prepare Paths\n",
    "s1_root = Path(\"../data/curriculum/stage1\")\n",
    "s1_mix_files = sorted(list((s1_root / \"mixture\").glob(\"*.npy\")))\n",
    "s1_tgt_files = sorted(list((s1_root / \"target\").glob(\"*.npy\")))\n",
    "\n",
    "# 2. Create Datasets (Using utils.SpectrogramDataset which loads raw waves)\n",
    "split_idx = int(len(s1_mix_files) * 0.8)\n",
    "train_ds_s1 = utils.SpectrogramDataset(s1_mix_files[:split_idx], s1_tgt_files[:split_idx])\n",
    "val_ds_s1 = utils.SpectrogramDataset(s1_mix_files[split_idx:], s1_tgt_files[split_idx:])\n",
    "\n",
    "train_loader_s1 = DataLoader(train_ds_s1, batch_size=batch_size, shuffle=True)\n",
    "val_loader_s1 = DataLoader(val_ds_s1, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. Check for Checkpoint\n",
    "stage1_ckpt = os.path.join(checkpoint_dir, \"model_a_stage1.pth\")\n",
    "\n",
    "if os.path.exists(stage1_ckpt):\n",
    "    print(f\"âœ“ Found Stage 1 checkpoint at {stage1_ckpt}. Loading...\")\n",
    "    model.load_state_dict(torch.load(stage1_ckpt, map_location=device))\n",
    "else:\n",
    "    # 4. Train\n",
    "    # We initialize the trainer. It handles the epoch loop and printing internally.\n",
    "    trainer_s1 = ma.ModelATrainer(\n",
    "        model, \n",
    "        train_loader_s1, \n",
    "        val_loader_s1, \n",
    "        processor, \n",
    "        learning_rate=learning_rate, \n",
    "        device=device, \n",
    "        patience=patience\n",
    "    )\n",
    "    \n",
    "    # This call will print progress epoch-by-epoch as they finish\n",
    "    history_s1 = trainer_s1.train(num_epochs=num_epochs)\n",
    "    \n",
    "    # Save Checkpoint\n",
    "    torch.save(model.state_dict(), stage1_ckpt)\n",
    "    \n",
    "    # Plot\n",
    "    if len(history_s1['train_loss']) > 0:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(history_s1['train_loss'], label='Train Loss')\n",
    "        plt.plot(history_s1['val_loss'], label='Val Loss')\n",
    "        plt.title(\"Stage 1 Training Results\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# STAGE 2 TRAINING (Full Mix -> Other)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸš€ STAGE 2: Fine-Tuning on Full Mixture\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Prepare Paths\n",
    "s2_root = Path(\"../data/curriculum/stage2\")\n",
    "s2_mix_files = sorted(list((s2_root / \"mixture\").glob(\"*.npy\")))\n",
    "s2_tgt_files = sorted(list((s2_root / \"target\").glob(\"*.npy\")))\n",
    "\n",
    "# 2. Create Datasets\n",
    "train_ds_s2 = utils.SpectrogramDataset(s2_mix_files[:split_idx], s2_tgt_files[:split_idx])\n",
    "val_ds_s2 = utils.SpectrogramDataset(s2_mix_files[split_idx:], s2_tgt_files[split_idx:])\n",
    "\n",
    "train_loader_s2 = DataLoader(train_ds_s2, batch_size=batch_size, shuffle=True)\n",
    "val_loader_s2 = DataLoader(val_ds_s2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 3. Check for Checkpoint\n",
    "stage2_ckpt = os.path.join(checkpoint_dir, \"model_a_stage2.pth\")\n",
    "\n",
    "if os.path.exists(stage2_ckpt):\n",
    "    print(f\"âœ“ Found Stage 2 checkpoint at {stage2_ckpt}. Loading...\")\n",
    "    model.load_state_dict(torch.load(stage2_ckpt, map_location=device))\n",
    "else:\n",
    "    # 4. Train (Fine-tune with lower learning rate)\n",
    "    trainer_s2 = ma.ModelATrainer(\n",
    "        model, \n",
    "        train_loader_s2, \n",
    "        val_loader_s2, \n",
    "        processor, \n",
    "        learning_rate=learning_rate * 0.1,  # Lower LR for fine-tuning\n",
    "        device=device, \n",
    "        patience=patience\n",
    "    )\n",
    "    \n",
    "    history_s2 = trainer_s2.train(num_epochs=num_epochs)\n",
    "    \n",
    "    # Save Checkpoint\n",
    "    torch.save(model.state_dict(), stage2_ckpt)\n",
    "    \n",
    "    # Plot\n",
    "    if len(history_s2['train_loss']) > 0:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(history_s2['train_loss'], label='Train Loss')\n",
    "        plt.plot(history_s2['val_loss'], label='Val Loss')\n",
    "        plt.title(\"Stage 2 Training Results\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b87990",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Inference\n",
    "\n",
    "- Run the trained model on test data\n",
    "\n",
    "- Visualize separated sources (waveforms, spectrograms)\n",
    "\n",
    "- Optionally, compute evaluation metrics (e.g., SDR, SIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves from a .pt checkpoint file\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to your checkpoint (update as needed)\n",
    "ckpt_path = \"../checkpoints/model_a_stage1.pth\"  # or model_a_stage2.pth\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "if 'history' in ckpt:\n",
    "    history = ckpt['history']\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(\"Loss Curves from Checkpoint\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No loss history found in checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4c3d6",
   "metadata": {},
   "source": [
    "## 7. Discussion and Next Steps\n",
    "\n",
    "- Summarize results\n",
    "\n",
    "- Suggest improvements or further experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2458ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-dl-env (Python 3.9.18)",
   "language": "python",
   "name": "final-dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
