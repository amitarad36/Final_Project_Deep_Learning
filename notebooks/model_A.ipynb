{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10b72ae",
   "metadata": {},
   "source": [
    "# Model A: Time-Frequency Domain U-Net for Music Source Separation\n",
    "\n",
    "This notebook demonstrates music source separation using a U-Net architecture in the time-frequency domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b5c35",
   "metadata": {},
   "source": [
    "## 1. Imports and Environment Setup\n",
    "\n",
    "- Import required libraries (torch, numpy, matplotlib, etc.)\n",
    "\n",
    "- Set device (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95784470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from models import utils, model_A as ma\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16be75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Define a log file path relative to the notebook's execution directory\n",
    "LOG_FILE_PATH = 'training_progress.log'\n",
    "if os.path.exists(LOG_FILE_PATH):\n",
    "    try:\n",
    "        os.remove(LOG_FILE_PATH)\n",
    "        print(f'Cleared old log file: {LOG_FILE_PATH}')\n",
    "    except Exception as e:\n",
    "        print(f'Error clearing old log file {LOG_FILE_PATH}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36209915",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "- Load example mixture and target data (from .npy files)\n",
    "\n",
    "- Visualize waveforms and spectrograms\n",
    "\n",
    "- Normalize or preprocess as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba2b4697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Data Cache...\n",
      "Cache found at ..\\data\\curriculum. Skipping generation.\n",
      "\n",
      " Data Ready!\n",
      "   Stage 1 Samples: 144\n",
      "   Stage 2 Samples: 144\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import musdb\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('..') \n",
    "from models import utils\n",
    "from models.utils import AudioProcessor\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. PREPARE CURRICULUM CACHE\n",
    "# ==============================================================================\n",
    "print(\"Checking Data Cache...\")\n",
    "mus = musdb.DB(download=True)\n",
    "utils.prepare_curriculum_cache(mus, cache_dir=\"../data/curriculum\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. LOAD DATA PATHS FOR BOTH STAGES\n",
    "# ==============================================================================\n",
    "data_root = Path(\"../data/curriculum\")\n",
    "s1_mix_path = data_root / \"stage1\" / \"mixture\"\n",
    "s1_tgt_path = data_root / \"stage1\" / \"target\"\n",
    "s2_mix_path = data_root / \"stage2\" / \"mixture\"\n",
    "s2_tgt_path = data_root / \"stage2\" / \"target\"\n",
    "\n",
    "mix_files_stage1 = sorted(list(s1_mix_path.glob(\"*.npy\")))\n",
    "tgt_files_stage1 = sorted(list(s1_tgt_path.glob(\"*.npy\")))\n",
    "mix_files_stage2 = sorted(list(s2_mix_path.glob(\"*.npy\")))\n",
    "tgt_files_stage2 = sorted(list(s2_tgt_path.glob(\"*.npy\")))\n",
    "\n",
    "print(f\"\\n Data Ready!\")\n",
    "print(f\"   Stage 1 Samples: {len(mix_files_stage1)}\")\n",
    "print(f\"   Stage 2 Samples: {len(mix_files_stage2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a129c",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "- Show the U-Net model code\n",
    "\n",
    "- Print model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "617cd60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeFrequencyDomainUNet(\n",
      "  (encoders): ModuleList(\n",
      "    (0): EncoderBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): EncoderBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): EncoderBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): EncoderBlock(\n",
      "      (block): Sequential(\n",
      "        (0): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): DecoderBlock(\n",
      "      (upconv): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (block): Sequential(\n",
      "        (0): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (upconv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (block): Sequential(\n",
      "        (0): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): DecoderBlock(\n",
      "      (upconv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (block): Sequential(\n",
      "        (0): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): DecoderBlock(\n",
      "      (upconv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (block): Sequential(\n",
      "        (0): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (1): ConvLayer2D(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): ConvLayer2D(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models import model_A as ma\n",
    "\n",
    "# Model summary (default architecture)\n",
    "model_summary = ma.TimeFrequencyDomainUNet(in_channels=1, out_channels=1, base_filters=64, num_layers=4).to(device)\n",
    "print(model_summary)\n",
    "del model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d10511",
   "metadata": {},
   "source": [
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2df9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete:\n",
      "   - Device: cpu\n",
      "   - Model: TimeFrequencyDomainUNet (Filters=64, Layers=4)\n",
      "   - Overfit Model: TimeFrequencyDomainUNet (Filters=128, Layers=4)\n",
      "   - Loss Function: MSELoss\n",
      "   - Optimizer: Adam (lr=0.0001)\n",
      "   - Overfit Optimizer: Adam (lr=0.0003)\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models import model_A as ma\n",
    "from models import utils\n",
    "\n",
    "# --- Configurable model and training parameters ---\n",
    "# These can be set externally (e.g., from main.ipynb) by injecting a config dict\n",
    "MODEL_CONFIG = globals().get('MODEL_CONFIG', {\n",
    "    'in_channels': 1,\n",
    "    'out_channels': 1,\n",
    "    'base_filters': 64,\n",
    "    'num_layers': 4,\n",
    "    'batchnorm': True,\n",
    "    'dropout': 0.1,\n",
    "})\n",
    "TRAIN_CONFIG = globals().get('TRAIN_CONFIG', {\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'patience': 10000,\n",
    "    'batch_size': 2,\n",
    "})\n",
    "OVERFIT_CONFIG = globals().get('OVERFIT_CONFIG', {\n",
    "    'base_filters': 128,\n",
    "    'num_layers': 4,\n",
    "    'batchnorm': True,\n",
    "    'dropout': 0.0,\n",
    "    'learning_rate': 3e-4,\n",
    "    'num_epochs': 100,\n",
    "    'batch_size': 2,\n",
    "    'patience': 10000,\n",
    "})\n",
    "\n",
    "# Device setup\n",
    "if 'device' not in globals():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if 'overfit_device' not in globals():\n",
    "    overfit_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "processor = utils.AudioProcessor(device=device)\n",
    "overfit_processor = utils.AudioProcessor(device=overfit_device)\n",
    "\n",
    "# Instantiate models\n",
    "model = ma.TimeFrequencyDomainUNet(\n",
    "    in_channels=MODEL_CONFIG['in_channels'],\n",
    "    out_channels=MODEL_CONFIG['out_channels'],\n",
    "    base_filters=MODEL_CONFIG['base_filters'],\n",
    "    num_layers=MODEL_CONFIG['num_layers'],\n",
    "    batchnorm=MODEL_CONFIG['batchnorm'],\n",
    "    dropout=MODEL_CONFIG['dropout']\n",
    ").to(device)\n",
    "overfit_model = ma.TimeFrequencyDomainUNet(\n",
    "    in_channels=MODEL_CONFIG['in_channels'],\n",
    "    out_channels=MODEL_CONFIG['out_channels'],\n",
    "    base_filters=OVERFIT_CONFIG['base_filters'],\n",
    "    num_layers=OVERFIT_CONFIG['num_layers'],\n",
    "    batchnorm=OVERFIT_CONFIG['batchnorm'],\n",
    "    dropout=OVERFIT_CONFIG['dropout']\n",
    ").to(overfit_device)\n",
    "\n",
    "# Optimizers and losses\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=TRAIN_CONFIG['learning_rate'])\n",
    "overfit_loss_fn = nn.MSELoss()\n",
    "overfit_optimizer = optim.Adam(overfit_model.parameters(), lr=OVERFIT_CONFIG['learning_rate'])\n",
    "\n",
    "print(f\"Setup Complete:\")\n",
    "print(f\"   - Device: {device}\")\n",
    "print(f\"   - Model: {model.__class__.__name__} (Filters={MODEL_CONFIG['base_filters']}, Layers={MODEL_CONFIG['num_layers']})\")\n",
    "print(f\"   - Overfit Model: {overfit_model.__class__.__name__} (Filters={OVERFIT_CONFIG['base_filters']}, Layers={OVERFIT_CONFIG['num_layers']})\")\n",
    "print(f\"   - Loss Function: {loss_fn.__class__.__name__}\")\n",
    "print(f\"   - Optimizer: {optimizer.__class__.__name__} (lr={TRAIN_CONFIG['learning_rate']})\")\n",
    "print(f\"   - Overfit Optimizer: {overfit_optimizer.__class__.__name__} (lr={OVERFIT_CONFIG['learning_rate']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78f629",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23765a5c",
   "metadata": {},
   "source": [
    "### 5.a Overfit Method\n",
    "\n",
    "- Train the model on a very small subset (e.g., one batch or a few samples) to ensure it can overfit and the implementation is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66b38e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Overfit Test on 5 Songs...\n",
      "✓ Found Checkpoint: ../checkpoints/debug_overfit_5songs.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amita\\AppData\\Local\\Temp\\ipykernel_102812\\1528628993.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(save_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "os.makedirs(\"../checkpoints\", exist_ok=True)\n",
    "\n",
    "print(\"Starting Overfit Test on 5 Songs...\")\n",
    "\n",
    "# Prepare small dataset for overfitting\n",
    "s1_root = Path(\"../data/curriculum/stage1\")\n",
    "mix_files = sorted(list((s1_root / \"mixture\").glob(\"*.npy\")))[:5]\n",
    "tgt_files = sorted(list((s1_root / \"target\").glob(\"*.npy\")))[:5]\n",
    "\n",
    "if len(mix_files) < 1 or len(tgt_files) < 1:\n",
    "    raise ValueError(\"Not enough data for overfit test. Please check your dataset.\")\n",
    "\n",
    "tiny_ds = utils.StandardDataset(mix_files, tgt_files)\n",
    "tiny_loader = DataLoader(tiny_ds, batch_size=OVERFIT_CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "trainer_overfit = utils.UniversalTrainer(\n",
    "    model=overfit_model,\n",
    "    train_loader=tiny_loader,\n",
    "    val_loader=tiny_loader,\n",
    "    processor=overfit_processor,\n",
    "    optimizer=overfit_optimizer,\n",
    "    loss_fn=overfit_loss_fn,\n",
    "    device=overfit_device,\n",
    "    patience=OVERFIT_CONFIG['patience']\n",
    ")\n",
    "\n",
    "save_path = \"../checkpoints/debug_overfit_5songs.pth\"\n",
    "history = {}\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    print(\"   Training from scratch on 5 songs...\")\n",
    "    history = trainer_overfit.train(num_epochs=OVERFIT_CONFIG['num_epochs'], save_path=save_path)\n",
    "else:\n",
    "    print(f\"✓ Found Checkpoint: {save_path}\")\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    overfit_model.load_state_dict(ckpt['model_state_dict'])\n",
    "    history = ckpt.get('history', {})\n",
    "\n",
    "# utils.plot_loss_history(history, \"Overfit Learning Curve (5 Songs)\")\n",
    "\n",
    "# Visualize results for each song (optional, can be commented out for OOM safety)\n",
    "# for i, batch in enumerate(tiny_loader):\n",
    "#     mix = batch['mix'].to(device)\n",
    "#     tgt = batch['tgt'].to(device)\n",
    "#     overfit_model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         mix_log, _ = processor.to_spectrogram(mix)\n",
    "#         tgt_log, _ = processor.to_spectrogram(tgt)\n",
    "#         mix_in = mix_log.unsqueeze(1)\n",
    "#         mask = overfit_model(mix_in)\n",
    "#         if mask.shape != mix_in.shape:\n",
    "#             mask = mask[:, :, :mix_in.shape[2], :mix_in.shape[3]]\n",
    "#         est_linear = mask * torch.expm1(mix_in)\n",
    "#         est_log = torch.log1p(est_linear)\n",
    "#     utils.visualize_results(mix_log[0], tgt_log[0], est_log[0].squeeze(), title=f\"Overfit Verification Song {i+1} (Log Scale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78860802",
   "metadata": {},
   "source": [
    "# Listen to mixture, target, and predicted audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e055393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.utils import play_audio\n",
    "#\n",
    "# sr = 22050\n",
    "# mix_wav = processor.to_waveform(mix_log[0].cpu(), torch.zeros_like(mix_log[0]))\n",
    "# tgt_wav = processor.to_waveform(tgt_log[0].cpu(), torch.zeros_like(tgt_log[0]))\n",
    "# est_wav = processor.to_waveform(est_log[0].cpu(), torch.zeros_like(est_log[0]))\n",
    "#\n",
    "# play_audio(mix_wav, sr=sr, title=\"Mixture Audio (Overfit)\")\n",
    "# play_audio(tgt_wav, sr=sr, title=\"Target Audio (Overfit)\")\n",
    "# play_audio(est_wav, sr=sr, title=\"Predicted Audio (Overfit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e972d",
   "metadata": {},
   "source": [
    "### 5.b Full Training\n",
    "\n",
    "- Train the model on the full training dataset as intended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43d255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Full Training Pipeline...\n",
      "\n",
      "--- Stage 1: Vocals + Other -> Other ---\n",
      "[DEBUG] Using tqdm.notebook for progress bars.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41d4ac4dfc6440a8699f6904ef8a517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Using tqdm.notebook for progress bars.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51a5882162042518907ce4d7566c0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ep 1 Training:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Starting Full Training Pipeline...\")\n",
    "\n",
    "# Stage 1\n",
    "print(\"\\n--- Stage 1: Vocals + Other -> Other ---\")\n",
    "\n",
    "s1_root = Path(\"../data/curriculum/stage1\")\n",
    "s1_mix = sorted(list((s1_root / \"mixture\").glob(\"*.npy\")))\n",
    "s1_tgt = sorted(list((s1_root / \"target\").glob(\"*.npy\")))\n",
    "\n",
    "split_s1 = int(len(s1_mix) * 0.8)\n",
    "train_ds1 = utils.StandardDataset(s1_mix[:split_s1], s1_tgt[:split_s1])\n",
    "val_ds1 = utils.StandardDataset(s1_mix[split_s1:], s1_tgt[split_s1:])\n",
    "\n",
    "if len(train_ds1) < 1 or len(val_ds1) < 1:\n",
    "    raise ValueError(\"Not enough data for full training Stage 1. Please check your dataset.\")\n",
    "\n",
    "train_loader1 = DataLoader(train_ds1, batch_size=TRAIN_CONFIG['batch_size'], shuffle=True)\n",
    "val_loader1 = DataLoader(val_ds1, batch_size=TRAIN_CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "trainer_s1 = utils.UniversalTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader1,\n",
    "    val_loader=val_loader1,\n",
    "    processor=processor,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    "    patience=TRAIN_CONFIG['patience']\n",
    ")\n",
    "\n",
    "path_s1 = \"../checkpoints/full_stage1.pth\"\n",
    "hist_s1 = {}\n",
    "\n",
    "if not os.path.exists(path_s1):\n",
    "    hist_s1 = trainer_s1.train(num_epochs=TRAIN_CONFIG['num_epochs'], save_path=path_s1, log_file_path=LOG_FILE_PATH)\n",
    "else:\n",
    "    print(f\"✓ Found Checkpoint: {path_s1}\")\n",
    "    ckpt = torch.load(path_s1, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    hist_s1 = ckpt.get('history', {})\n",
    "\n",
    "# utils.plot_loss_history(hist_s1, \"Stage 1 Results\")\n",
    "\n",
    "# Stage 2\n",
    "print(\"\\n--- Stage 2: Full Mix -> Other ---\")\n",
    "\n",
    "s2_root = Path(\"../data/curriculum/stage2\")\n",
    "s2_mix = sorted(list((s2_root / \"mixture\").glob(\"*.npy\")))\n",
    "s2_tgt = sorted(list((s2_root / \"target\").glob(\"*.npy\")))\n",
    "\n",
    "split_s2 = int(len(s2_mix) * 0.8)\n",
    "train_ds2 = utils.StandardDataset(s2_mix[:split_s2], s2_tgt[:split_s2])\n",
    "val_ds2 = utils.StandardDataset(s2_mix[split_s2:], s2_tgt[split_s2:])\n",
    "\n",
    "if len(train_ds2) < 1 or len(val_ds2) < 1:\n",
    "    raise ValueError(\"Not enough data for full training Stage 2. Please check your dataset.\")\n",
    "\n",
    "train_loader2 = DataLoader(train_ds2, batch_size=TRAIN_CONFIG['batch_size'], shuffle=True)\n",
    "val_loader2 = DataLoader(val_ds2, batch_size=TRAIN_CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = TRAIN_CONFIG['learning_rate'] * 0.1\n",
    "print(f\"✓ Optimizer LR reduced to {TRAIN_CONFIG['learning_rate'] * 0.1}\")\n",
    "\n",
    "trainer_s2 = utils.UniversalTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader2,\n",
    "    val_loader=val_loader2,\n",
    "    processor=processor,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    "    patience=TRAIN_CONFIG['patience']\n",
    ")\n",
    "\n",
    "path_s2 = \"../checkpoints/full_stage2.pth\"\n",
    "hist_s2 = {}\n",
    "\n",
    "if not os.path.exists(path_s2):\n",
    "    hist_s2 = trainer_s2.train(num_epochs=TRAIN_CONFIG['num_epochs'], save_path=path_s2, log_file_path=LOG_FILE_PATH)\n",
    "else:\n",
    "    print(f\"✓ Found Checkpoint: {path_s2}\")\n",
    "    ckpt = torch.load(path_s2, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    hist_s2 = ckpt.get('history', {})\n",
    "\n",
    "# utils.plot_loss_history(hist_s2, \"Stage 2 Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b87990",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Inference\n",
    "\n",
    "- Run the trained model on test data\n",
    "\n",
    "- Visualize separated sources (waveforms, spectrograms)\n",
    "\n",
    "- Optionally, compute evaluation metrics (e.g., SDR, SIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ckpt_path = \"../checkpoints/full_stage2.pth\"\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "if 'history' in ckpt:\n",
    "    history = ckpt['history']\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(\"Loss Curves from Checkpoint\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No loss history found in checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4c3d6",
   "metadata": {},
   "source": [
    "## 7. Listen to the masked songs\n",
    "\n",
    "- listen to masked songs from the MUSDB18 dataset\n",
    "\n",
    "- watch the spectrograms\n",
    "\n",
    "- upload a song to model_A_input folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from models.utils import show_spectrogram, play_audio, AudioProcessor\n",
    "from models.model_A import TimeFrequencyDomainUNet\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Parameters\n",
    "sr = 22050 # Sample rate\n",
    "duration = 6 # seconds\n",
    "n_samples = sr * duration\n",
    "\n",
    "# Load a sample from MUSDB18 cache\n",
    "data_root = Path(\"../data/curriculum\")\n",
    "s1_mix_path = data_root / \"stage1\" / \"mixture\"\n",
    "s1_tgt_path = data_root / \"stage1\" / \"target\"\n",
    "mix_files = sorted(list(s1_mix_path.glob(\"*.npy\")))\n",
    "tgt_files = sorted(list(s1_tgt_path.glob(\"*.npy\")))\n",
    "song_num = 12  # Change this to select a different sample from dataset musdb18\n",
    "\n",
    "sample_idx = song_num\n",
    "mix_wav = np.load(mix_files[sample_idx])[:n_samples]\n",
    "tgt_wav = np.load(tgt_files[sample_idx])[:n_samples]\n",
    "\n",
    "# Compute and show spectrograms for 6 sec sample\n",
    "processor = AudioProcessor(device=device)\n",
    "mix_mag, mix_phase = processor.to_spectrogram(torch.tensor(mix_wav))\n",
    "tgt_mag, tgt_phase = processor.to_spectrogram(torch.tensor(tgt_wav))\n",
    "show_spectrogram(mix_mag, title=\"Mixture Spectrogram (6 sec)\")\n",
    "show_spectrogram(tgt_mag, title=\"Target Spectrogram (6 sec)\")\n",
    "\n",
    "# Predict masked output and show predicted spectrogram\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Ensure mix_mag_in is 4D: (batch, channel, height, width)\n",
    "    if mix_mag.dim() == 2:\n",
    "        mix_mag_in = mix_mag.unsqueeze(0).unsqueeze(0).to(device)  # (1, 1, H, W)\n",
    "    elif mix_mag.dim() == 3:\n",
    "        mix_mag_in = mix_mag.unsqueeze(1).to(device)  # (batch, 1, H, W)\n",
    "    elif mix_mag.dim() == 4:\n",
    "        mix_mag_in = mix_mag.to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"mix_mag must be 2D, 3D, or 4D, got shape {mix_mag.shape}\")\n",
    "\n",
    "    if mix_mag_in.dim() != 4:\n",
    "        raise ValueError(f\"mix_mag_in must be 4D, got shape {mix_mag_in.shape}\")\n",
    "\n",
    "    mask = model(mix_mag_in)\n",
    "    if mask.shape != mix_mag_in.shape:\n",
    "        mask = mask[:, :, :mix_mag_in.shape[2], :mix_mag_in.shape[3]]\n",
    "    est_mag = mask.squeeze(0).squeeze(0) * mix_mag.to(device)\n",
    "    est_wav = processor.to_waveform(est_mag.cpu(), mix_phase.cpu())\n",
    "show_spectrogram(est_mag.cpu(), title=\"Predicted Spectrogram (6 sec)\")\n",
    "\n",
    "# Play audio (optional, can comment out if not needed)\n",
    "play_audio(mix_wav, sr=sr, title=\"Mixture Audio (6 sec)\")\n",
    "play_audio(tgt_wav, sr=sr, title=\"Target Audio (6 sec)\")\n",
    "play_audio(est_wav, sr=sr, title=\"Predicted Audio (6 sec)\")\n",
    "\n",
    "# Optionally upload and process your own song\n",
    "input_dir = Path(\"../model_A_input\")\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "user_files = sorted(list(input_dir.glob(\"*.wav\")))\n",
    "if user_files:\n",
    "    user_path = user_files[0] # Take the first uploaded music file \n",
    "    user_wav, user_sr = librosa.load(user_path, sr=sr)\n",
    "    user_wav = user_wav[:n_samples]\n",
    "    user_mag, user_phase = processor.to_spectrogram(torch.tensor(user_wav))\n",
    "    show_spectrogram(user_mag, title=f\"User Mixture Spectrogram (6 sec): {user_path.name}\")\n",
    "    with torch.no_grad():\n",
    "        user_mag_in = user_mag.unsqueeze(0).unsqueeze(1).to(device)  # Shape: (1, 1, H, W)\n",
    "        if user_mag_in.dim() != 4:\n",
    "            raise ValueError(f\"user_mag_in must be 4D, got shape {user_mag_in.shape}\")\n",
    "        user_mask = model(user_mag_in)\n",
    "        if user_mask.shape != user_mag_in.shape:\n",
    "            user_mask = user_mask[:, :, :user_mag_in.shape[2], :user_mag_in.shape[3]]\n",
    "        user_est_mag = user_mask.squeeze(0).squeeze(0) * user_mag.to(device)\n",
    "        user_est_wav = processor.to_waveform(user_est_mag.cpu(), user_phase.cpu())\n",
    "    show_spectrogram(user_est_mag.cpu(), title=f\"User Predicted Spectrogram (6 sec): {user_path.name}\")\n",
    "    show_spectrogram(user_phase.cpu() if hasattr(user_phase, 'cpu') else user_phase, title=f\"User Phase Spectrogram (6 sec): {user_path.name}\")\n",
    "    # play_audio(user_wav, sr=sr, title=f\"User Mixture Audio (6 sec): {user_path.name}\")\n",
    "    # play_audio(user_est_wav, sr=sr, title=f\"User Predicted Audio (6 sec): {user_path.name}\")\n",
    "else:\n",
    "    print(\"No user .wav file found in model_A_input. Upload a song to try your own audio!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919dfff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "print('Current working directory:', os.getcwd())\n",
    "print('sys.path:', sys.path)\n",
    "print('Directory listing:', os.listdir('.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
