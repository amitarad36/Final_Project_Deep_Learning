{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10b72ae",
   "metadata": {},
   "source": [
    "# Model A: Time-Frequency Domain U-Net for Music Source Separation\n",
    "\n",
    "This notebook demonstrates music source separation using a U-Net architecture in the time-frequency domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b5c35",
   "metadata": {},
   "source": [
    "## 1. Imports and Environment Setup\n",
    "\n",
    "- Import required libraries (torch, numpy, matplotlib, etc.)\n",
    "\n",
    "- Set device (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95784470",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from models import utils, model_A as ma\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36209915",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "- Load example mixture and target data (from .npy files)\n",
    "\n",
    "- Visualize waveforms and spectrograms\n",
    "\n",
    "- Normalize or preprocess as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import musdb\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('..') \n",
    "from models import utils\n",
    "from models.utils import AudioProcessor\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. PREPARE CURRICULUM CACHE\n",
    "# ==============================================================================\n",
    "print(\"Checking Data Cache...\")\n",
    "mus = musdb.DB(download=True)\n",
    "utils.prepare_curriculum_cache(mus, cache_dir=\"../data/curriculum\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. LOAD DATA PATHS FOR BOTH STAGES\n",
    "# ==============================================================================\n",
    "data_root = Path(\"../data/curriculum\")\n",
    "s1_mix_path = data_root / \"stage1\" / \"mixture\"\n",
    "s1_tgt_path = data_root / \"stage1\" / \"target\"\n",
    "s2_mix_path = data_root / \"stage2\" / \"mixture\"\n",
    "s2_tgt_path = data_root / \"stage2\" / \"target\"\n",
    "\n",
    "mix_files_stage1 = sorted(list(s1_mix_path.glob(\"*.npy\")))\n",
    "tgt_files_stage1 = sorted(list(s1_tgt_path.glob(\"*.npy\")))\n",
    "mix_files_stage2 = sorted(list(s2_mix_path.glob(\"*.npy\")))\n",
    "tgt_files_stage2 = sorted(list(s2_tgt_path.glob(\"*.npy\")))\n",
    "\n",
    "print(f\"\\n Data Ready!\")\n",
    "print(f\"   Stage 1 Samples: {len(mix_files_stage1)}\")\n",
    "print(f\"   Stage 2 Samples: {len(mix_files_stage2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88a129c",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "- Show the U-Net model code\n",
    "\n",
    "- Print model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617cd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import model_A as ma\n",
    "\n",
    "# Model summary (default architecture)\n",
    "model_summary = ma.TimeFrequencyDomainUNet(in_channels=1, out_channels=1, base_filters=64, num_layers=4).to(device)\n",
    "print(model_summary)\n",
    "del model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d10511",
   "metadata": {},
   "source": [
    "## 4. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2df9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models import model_A as ma\n",
    "from models import utils\n",
    "\n",
    "# Model hyperparameters\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "base_filters = 32\n",
    "num_layers = 4\n",
    "batchnorm = True\n",
    "dropout = 0.0\n",
    "\n",
    "# Overfit model hyperparameters\n",
    "overfit_base_filters = 128\n",
    "overfit_num_layers = 4\n",
    "overfit_batchnorm = True\n",
    "overfit_dropout = 0.0\n",
    "overfit_learning_rate = 3e-4\n",
    "overfit_num_epochs = 100\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 1e-4\n",
    "patience = 5\n",
    "batch_size = 8\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "processor = utils.AudioProcessor(device=device)\n",
    "\n",
    "# Instantiate models\n",
    "model = ma.TimeFrequencyDomainUNet(in_channels=in_channels, out_channels=out_channels, base_filters=base_filters, num_layers=num_layers, batchnorm=batchnorm, dropout=dropout).to(device)\n",
    "overfit_model = ma.TimeFrequencyDomainUNet(in_channels=in_channels, out_channels=out_channels, base_filters=overfit_base_filters, num_layers=overfit_num_layers, batchnorm=overfit_batchnorm, dropout=overfit_dropout).to(device)\n",
    "\n",
    "# Optimizers and losses\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "overfit_loss_fn = nn.MSELoss()\n",
    "overfit_optimizer = optim.Adam(overfit_model.parameters(), lr=overfit_learning_rate)\n",
    "\n",
    "print(f\"Setup Complete:\")\n",
    "print(f\"   - Device: {device}\")\n",
    "print(f\"   - Model: {model.__class__.__name__} (Filters={base_filters}, Layers={num_layers})\")\n",
    "print(f\"   - Overfit Model: {overfit_model.__class__.__name__} (Filters={overfit_base_filters}, Layers={overfit_num_layers})\")\n",
    "print(f\"   - Loss Function: {loss_fn.__class__.__name__}\")\n",
    "print(f\"   - Optimizer: {optimizer.__class__.__name__} (lr={learning_rate})\")\n",
    "print(f\"   - Overfit Optimizer: {overfit_optimizer.__class__.__name__} (lr={overfit_learning_rate})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78f629",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23765a5c",
   "metadata": {},
   "source": [
    "### 5.a Overfit Method\n",
    "\n",
    "- Train the model on a very small subset (e.g., one batch or a few samples) to ensure it can overfit and the implementation is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "os.makedirs(\"../checkpoints\", exist_ok=True)\n",
    "\n",
    "print(\"Starting Overfit Test...\")\n",
    "\n",
    "# Prepare tiny dataset\n",
    "s1_root = Path(\"../data/curriculum/stage1\")\n",
    "mix_files = sorted(list((s1_root / \"mixture\").glob(\"*.npy\")))[:1]\n",
    "tgt_files = sorted(list((s1_root / \"target\").glob(\"*.npy\")))[:1]\n",
    "\n",
    "tiny_ds = utils.StandardDataset(mix_files, tgt_files)\n",
    "tiny_loader = DataLoader(tiny_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "trainer_overfit = utils.UniversalTrainer(\n",
    "    model=overfit_model,\n",
    "    train_loader=tiny_loader,\n",
    "    val_loader=tiny_loader,\n",
    "    processor=processor,\n",
    "    optimizer=overfit_optimizer,\n",
    "    loss_fn=overfit_loss_fn,\n",
    "    device=device,\n",
    "    patience=overfit_num_epochs\n",
    ")\n",
    "\n",
    "save_path = \"../checkpoints/debug_overfit.pth\"\n",
    "history = {}\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    print(\"   Training from scratch...\")\n",
    "    history = trainer_overfit.train(num_epochs=overfit_num_epochs, save_path=save_path)\n",
    "else:\n",
    "    print(f\"✓ Found Checkpoint: {save_path}\")\n",
    "    ckpt = torch.load(save_path, map_location=device)\n",
    "    overfit_model.load_state_dict(ckpt['model_state_dict'])\n",
    "    history = ckpt.get('history', {})\n",
    "\n",
    "utils.plot_loss_history(history, \"Overfit Learning Curve\")\n",
    "\n",
    "batch = next(iter(tiny_loader))\n",
    "mix = batch['mix'].to(device)\n",
    "tgt = batch['tgt'].to(device)\n",
    "\n",
    "overfit_model.eval()\n",
    "with torch.no_grad():\n",
    "    mix_log, _ = processor.to_spectrogram(mix)\n",
    "    tgt_log, _ = processor.to_spectrogram(tgt)\n",
    "    mix_in = mix_log.unsqueeze(1)\n",
    "    mask = overfit_model(mix_in)\n",
    "    if mask.shape != mix_in.shape:\n",
    "        mask = mask[:, :, :mix_in.shape[2], :mix_in.shape[3]]\n",
    "    est_linear = mask * torch.expm1(mix_in)\n",
    "    est_log = torch.log1p(est_linear)\n",
    "\n",
    "utils.visualize_results(mix_log[0], tgt_log[0], est_log[0].squeeze(), title=\"Overfit Verification (Log Scale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78860802",
   "metadata": {},
   "source": [
    "# Listen to mixture, target, and predicted audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e055393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.utils import play_audio\n",
    "\n",
    "sr = 22050\n",
    "\n",
    "mix_wav = processor.to_waveform(mix_log[0].cpu(), torch.zeros_like(mix_log[0]))\n",
    "tgt_wav = processor.to_waveform(tgt_log[0].cpu(), torch.zeros_like(tgt_log[0]))\n",
    "est_wav = processor.to_waveform(est_log[0].cpu(), torch.zeros_like(est_log[0]))\n",
    "\n",
    "play_audio(mix_wav, sr=sr, title=\"Mixture Audio (Overfit)\")\n",
    "play_audio(tgt_wav, sr=sr, title=\"Target Audio (Overfit)\")\n",
    "play_audio(est_wav, sr=sr, title=\"Predicted Audio (Overfit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e972d",
   "metadata": {},
   "source": [
    "### 5.b Full Training\n",
    "\n",
    "- Train the model on the full training dataset as intended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Full Training Pipeline...\")\n",
    "\n",
    "# Stage 1\n",
    "print(\"\\n--- Stage 1: Vocals + Other -> Other ---\")\n",
    "\n",
    "s1_root = Path(\"../data/curriculum/stage1\")\n",
    "s1_mix = sorted(list((s1_root / \"mixture\").glob(\"*.npy\")))\n",
    "s1_tgt = sorted(list((s1_root / \"target\").glob(\"*.npy\")))\n",
    "\n",
    "split_s1 = int(len(s1_mix) * 0.8)\n",
    "train_ds1 = utils.StandardDataset(s1_mix[:split_s1], s1_tgt[:split_s1])\n",
    "val_ds1 = utils.StandardDataset(s1_mix[split_s1:], s1_tgt[split_s1:])\n",
    "\n",
    "train_loader1 = DataLoader(train_ds1, batch_size=batch_size, shuffle=True)\n",
    "val_loader1 = DataLoader(val_ds1, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "trainer_s1 = utils.UniversalTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader1,\n",
    "    val_loader=val_loader1,\n",
    "    processor=processor,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    "    patience=patience\n",
    ")\n",
    "\n",
    "path_s1 = \"../checkpoints/full_stage1.pth\"\n",
    "hist_s1 = {}\n",
    "\n",
    "if not os.path.exists(path_s1):\n",
    "    hist_s1 = trainer_s1.train(num_epochs=2, save_path=path_s1)\n",
    "else:\n",
    "    print(f\"✓ Found Checkpoint: {path_s1}\")\n",
    "    ckpt = torch.load(path_s1, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    hist_s1 = ckpt.get('history', {})\n",
    "\n",
    "utils.plot_loss_history(hist_s1, \"Stage 1 Results\")\n",
    "\n",
    "print(\"\\nVisualizing Stage 1 Spectrograms...\")\n",
    "sample_batch = next(iter(val_loader1))\n",
    "mix = sample_batch['mix'].to(device)\n",
    "tgt = sample_batch['tgt'].to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mix_mag, _ = processor.to_spectrogram(mix)\n",
    "    tgt_mag, _ = processor.to_spectrogram(tgt)\n",
    "    mix_in = mix_mag.unsqueeze(1)\n",
    "    mask = model(mix_in)\n",
    "    if mask.shape != mix_in.shape:\n",
    "        mask = mask[:, :, :mix_in.shape[2], :mix_in.shape[3]]\n",
    "    est_mag = mask * torch.expm1(mix_in)\n",
    "utils.visualize_results(mix_mag[0], tgt_mag[0], est_mag[0].squeeze(), title=\"Stage 1: Mixture vs Target vs Prediction\")\n",
    "\n",
    "# Stage 2\n",
    "print(\"\\n--- Stage 2: Full Mix -> Other ---\")\n",
    "\n",
    "s2_root = Path(\"../data/curriculum/stage2\")\n",
    "s2_mix = sorted(list((s2_root / \"mixture\").glob(\"*.npy\")))\n",
    "s2_tgt = sorted(list((s2_root / \"target\").glob(\"*.npy\")))\n",
    "\n",
    "split_s2 = int(len(s2_mix) * 0.8)\n",
    "train_ds2 = utils.StandardDataset(s2_mix[:split_s2], s2_tgt[:split_s2])\n",
    "val_ds2 = utils.StandardDataset(s2_mix[split_s2:], s2_tgt[split_s2:])\n",
    "\n",
    "train_loader2 = DataLoader(train_ds2, batch_size=batch_size, shuffle=True)\n",
    "val_loader2 = DataLoader(val_ds2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = learning_rate * 0.1\n",
    "print(f\"✓ Optimizer LR reduced to {learning_rate * 0.1}\")\n",
    "\n",
    "trainer_s2 = utils.UniversalTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader2,\n",
    "    val_loader=val_loader2,\n",
    "    processor=processor,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    "    patience=patience\n",
    ")\n",
    "\n",
    "path_s2 = \"../checkpoints/full_stage2.pth\"\n",
    "hist_s2 = {}\n",
    "\n",
    "if not os.path.exists(path_s2):\n",
    "    hist_s2 = trainer_s2.train(num_epochs=2, save_path=path_s2)\n",
    "else:\n",
    "    print(f\"✓ Found Checkpoint: {path_s2}\")\n",
    "    ckpt = torch.load(path_s2, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    hist_s2 = ckpt.get('history', {})\n",
    "\n",
    "utils.plot_loss_history(hist_s2, \"Stage 2 Results\")\n",
    "\n",
    "print(\"\\nVisualizing Stage 2 Spectrograms...\")\n",
    "sample_batch = next(iter(val_loader2))\n",
    "mix = sample_batch['mix'].to(device)\n",
    "tgt = sample_batch['tgt'].to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mix_mag, _ = processor.to_spectrogram(mix)\n",
    "    tgt_mag, _ = processor.to_spectrogram(tgt)\n",
    "    mix_in = mix_mag.unsqueeze(1)\n",
    "    mask = model(mix_in)\n",
    "    if mask.shape != mix_in.shape:\n",
    "        mask = mask[:, :, :mix_in.shape[2], :mix_in.shape[3]]\n",
    "    est_mag = mask * torch.expm1(mix_in)\n",
    "utils.visualize_results(mix_mag[0], tgt_mag[0], est_mag[0].squeeze(), title=\"Stage 2: Mixture vs Target vs Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b87990",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Inference\n",
    "\n",
    "- Run the trained model on test data\n",
    "\n",
    "- Visualize separated sources (waveforms, spectrograms)\n",
    "\n",
    "- Optionally, compute evaluation metrics (e.g., SDR, SIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ckpt_path = \"../checkpoints/model_a_stage1.pth\"\n",
    "\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "if 'history' in ckpt:\n",
    "    history = ckpt['history']\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(\"Loss Curves from Checkpoint\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No loss history found in checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4c3d6",
   "metadata": {},
   "source": [
    "## 7. Listen to the masked songs\n",
    "\n",
    "- listen to masked songs from the MUSDB18 dataset\n",
    "\n",
    "- watch the spectrograms\n",
    "\n",
    "- upload a song to model_A_input folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from models.utils import show_spectrogram, play_audio, AudioProcessor\n",
    "from models.model_A import TimeFrequencyDomainUNet\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# Load a sample from MUSDB18 cache\n",
    "data_root = Path(\"../data/curriculum\")\n",
    "s1_mix_path = data_root / \"stage1\" / \"mixture\"\n",
    "s1_tgt_path = data_root / \"stage1\" / \"target\"\n",
    "mix_files = sorted(list(s1_mix_path.glob(\"*.npy\")))\n",
    "tgt_files = sorted(list(s1_tgt_path.glob(\"*.npy\")))\n",
    "song_num = 36  # Change this to select a different sample frrom dataset musdb18\n",
    "\n",
    "sample_idx = song_num\n",
    "mix_wav = np.load(mix_files[sample_idx])\n",
    "tgt_wav = np.load(tgt_files[sample_idx])\n",
    "sr = 22050 # Sample rate\n",
    "\n",
    "# Compute and show spectrograms\n",
    "processor = AudioProcessor(device=device)\n",
    "mix_mag, mix_phase = processor.to_spectrogram(torch.tensor(mix_wav))\n",
    "tgt_mag, tgt_phase = processor.to_spectrogram(torch.tensor(tgt_wav))\n",
    "show_spectrogram(mix_mag, title=\"Mixture Spectrogram\")\n",
    "show_spectrogram(tgt_mag, title=\"Target Spectrogram\")\n",
    "\n",
    "# Predict masked output\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mix_mag_in = mix_mag.unsqueeze(0).unsqueeze(1).to(device)\n",
    "    mask = model(mix_mag_in)\n",
    "    if mask.shape != mix_mag_in.shape:\n",
    "        mask = mask[:, :, :mix_mag_in.shape[2], :mix_mag_in.shape[3]]\n",
    "    est_mag = mask.squeeze(0).squeeze(0) * mix_mag.to(device)\n",
    "    est_wav = processor.to_waveform(est_mag.cpu(), mix_phase.cpu())\n",
    "show_spectrogram(est_mag.cpu(), title=\"Predicted Spectrogram\")\n",
    "\n",
    "# Play audio\n",
    "play_audio(mix_wav, sr=sr, title=\"Mixture Audio\")\n",
    "play_audio(tgt_wav, sr=sr, title=\"Target Audio\")\n",
    "play_audio(est_wav, sr=sr, title=\"Predicted Audio\")\n",
    "\n",
    "# Optionally upload and process your own song\n",
    "input_dir = Path(\"../model_A_input\")\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "user_files = sorted(list(input_dir.glob(\"*.wav\")))\n",
    "if user_files:\n",
    "    user_path = user_files[0] # Take the first uploaded music file \n",
    "    user_wav, user_sr = librosa.load(user_path, sr=sr)\n",
    "    user_mag, user_phase = processor.to_spectrogram(torch.tensor(user_wav))\n",
    "    show_spectrogram(user_mag, title=f\"User Mixture Spectrogram: {user_path.name}\")\n",
    "    with torch.no_grad():\n",
    "        user_mag_in = user_mag.unsqueeze(0).unsqueeze(1).to(device)\n",
    "        user_mask = model(user_mag_in)\n",
    "        if user_mask.shape != user_mag_in.shape:\n",
    "            user_mask = user_mask[:, :, :user_mag_in.shape[2], :user_mag_in.shape[3]]\n",
    "        user_est_mag = user_mask.squeeze(0).squeeze(0) * user_mag.to(device)\n",
    "        user_est_wav = processor.to_waveform(user_est_mag.cpu(), user_phase.cpu())\n",
    "    show_spectrogram(user_est_mag.cpu(), title=f\"User Predicted Spectrogram: {user_path.name}\")\n",
    "    play_audio(user_wav, sr=sr, title=f\"User Mixture Audio: {user_path.name}\")\n",
    "    play_audio(user_est_wav, sr=sr, title=f\"User Predicted Audio: {user_path.name}\")\n",
    "else:\n",
    "    print(\"No user .wav file found in model_A_input. Upload a song to try your own audio!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-dl-env (Python 3.9.18)",
   "language": "python",
   "name": "final-dl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
