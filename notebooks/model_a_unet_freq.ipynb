{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10b72ae",
   "metadata": {},
   "source": [
    "# Model A: Curriculum Learning - Other/Piano Extraction\n",
    "\n",
    "**Curriculum Learning Strategy:**\n",
    "1. **Stage 1:** Extract \"other\" from simplified mixture (vocals + other only)\n",
    "2. **Stage 2:** Extract \"other\" from full mixture (drums + bass + vocals + other)\n",
    "\n",
    "**MUSDB18 Dataset:** 4 stems per track (drums, bass, other, vocals)\n",
    "\n",
    "**Workflow:**\n",
    "- Load MUSDB18 ‚Üí prepare curriculum batches\n",
    "- Train Stage 1 on simpler 2-source task\n",
    "- Train Stage 2 on full 4-source mixture using Stage 1 weights\n",
    "- Test on uploaded song (10 seconds from 1:00-1:10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c340beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Setup paths\n",
    "project_root = Path(os.getcwd()).resolve()\n",
    "if project_root.name.lower() == \"notebooks\":\n",
    "    project_root = project_root.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "checkpoints_dir = project_root / \"checkpoints\"\n",
    "data_dir = project_root / \"data\"\n",
    "\n",
    "checkpoints_dir.mkdir(exist_ok=True, parents=True)\n",
    "data_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"‚úì Setup complete | Device: {device} | Project: {project_root}\")\n",
    "\n",
    "# Import model components\n",
    "from models.model_a_unet_freq import (\n",
    "    STFTProcessor, FrequencyDomainUNet, \n",
    "    SourceSeparationDataset, ModelATrainer, ModelAInference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ac252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MUSDB18 dataset - Auto download if not present\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING MUSDB18 DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    import musdb\n",
    "    print(\"‚úì musdb library found\")\n",
    "except ImportError:\n",
    "    print(\"Installing musdb...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"musdb\", \"-q\"])\n",
    "    import musdb\n",
    "\n",
    "print(\"\\nLoading MUSDB18 (auto-downloading if needed)...\")\n",
    "print(\"Note: First run may take time. Dataset will be cached for future use.\\n\")\n",
    "\n",
    "try:\n",
    "    mus = musdb.DB(download=True)\n",
    "    tracks = mus.tracks\n",
    "    print(f\"‚úì MUSDB18 loaded successfully!\")\n",
    "    print(f\"‚úì Available tracks: {len(tracks)}\")\n",
    "    use_real_musdb = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load MUSDB18: {e}\")\n",
    "    print(\"Will use synthetic data instead\")\n",
    "    mus = None\n",
    "    use_real_musdb = False\n",
    "\n",
    "# MUSDB18 stems: [0]=drums, [1]=bass, [2]=other, [3]=vocals\n",
    "STEM_NAMES = {0: 'drums', 1: 'bass', 2: 'other', 3: 'vocals'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a1c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual MUSDB18 Download Instructions\n",
    "if not use_real_musdb:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MUSDB18 DATASET REQUIRED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nThe musdb library doesn't support automatic downloads.\")\n",
    "    print(\"Please follow these steps to download MUSDB18:\\n\")\n",
    "    print(\"1. Visit: https://sigsep.github.io/datasets/musdb.html\")\n",
    "    print(\"2. Download the MUSDB18-HQ dataset (~23GB)\")\n",
    "    print(\"3. Extract the ZIP file to:\")\n",
    "    print(f\"   {musdb_root}\")\n",
    "    print(\"\\n4. After extraction, the structure should be:\")\n",
    "    print(f\"   {musdb_root}/\")\n",
    "    print(f\"     ‚îú‚îÄ‚îÄ train/\")\n",
    "    print(f\"     ‚îÇ   ‚îú‚îÄ‚îÄ A Classic Education - NightOwl/\")\n",
    "    print(f\"     ‚îÇ   ‚îú‚îÄ‚îÄ ...\")\n",
    "    print(f\"     ‚îî‚îÄ‚îÄ test/\")\n",
    "    print(f\"         ‚îú‚îÄ‚îÄ ...\")\n",
    "    print(\"\\n5. Then re-run cells 2-3 to detect the dataset\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "else:\n",
    "    print(\"‚úì MUSDB18 already available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb748fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare curriculum learning data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CURRICULUM LEARNING DATA PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def prepare_curriculum_data(num_tracks=50):\n",
    "    \"\"\"Prepare data for curriculum learning using MUSDB18\"\"\"\n",
    "    \n",
    "    if not use_real_musdb or mus is None:\n",
    "        raise ValueError(\n",
    "            \"MUSDB18 dataset could not be loaded.\\n\"\n",
    "            \"Please check your internet connection and try again.\\n\"\n",
    "            \"The musdb library will attempt to download automatically.\"\n",
    "        )\n",
    "    \n",
    "    tracks = mus.tracks[:num_tracks]\n",
    "    print(f\"\\nProcessing {len(tracks)} MUSDB18 tracks for curriculum learning...\")\n",
    "    \n",
    "    stage1_mixture_paths = []\n",
    "    stage1_target_paths = []\n",
    "    stage2_mixture_paths = []\n",
    "    stage2_target_paths = []\n",
    "    \n",
    "    cache_dir = data_dir / \"curriculum_cache\"\n",
    "    cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    for idx, track in enumerate(tracks):\n",
    "        try:\n",
    "            # Extract stems\n",
    "            drums = track.targets['drums'].audio\n",
    "            bass = track.targets['bass'].audio\n",
    "            other = track.targets['other'].audio\n",
    "            vocals = track.targets['vocals'].audio\n",
    "            \n",
    "            # Create mixtures\n",
    "            # Stage 1: vocals + other (simplified)\n",
    "            mixture_s1 = vocals + other\n",
    "            # Stage 2: drums + bass + other + vocals (full)\n",
    "            mixture_s2 = drums + bass + other + vocals\n",
    "            \n",
    "            # Resample to 22050 Hz if needed\n",
    "            # Use MUSDB18 default sample rate (44100 Hz)\n",
    "            sr = getattr(track, 'sample_rate', None) or 44100\n",
    "            \n",
    "            if sr != 22050:\n",
    "                from scipy import signal\n",
    "                n_samples = int(len(other) * 22050 / sr)\n",
    "                other = signal.resample(other, n_samples)\n",
    "                mixture_s1 = signal.resample(mixture_s1, n_samples)\n",
    "                mixture_s2 = signal.resample(mixture_s2, n_samples)\n",
    "            \n",
    "            # Convert stereo to mono\n",
    "            if other.ndim > 1:\n",
    "                other = np.mean(other, axis=1)\n",
    "            if mixture_s1.ndim > 1:\n",
    "                mixture_s1 = np.mean(mixture_s1, axis=1)\n",
    "            if mixture_s2.ndim > 1:\n",
    "                mixture_s2 = np.mean(mixture_s2, axis=1)\n",
    "            \n",
    "            # Normalize\n",
    "            other = other / (np.max(np.abs(other)) + 1e-8)\n",
    "            mixture_s1 = mixture_s1 / (np.max(np.abs(mixture_s1)) + 1e-8)\n",
    "            mixture_s2 = mixture_s2 / (np.max(np.abs(mixture_s2)) + 1e-8)\n",
    "            \n",
    "            # Save files\n",
    "            s1_mix_path = cache_dir / f\"stage1_mixture_{idx:03d}.npy\"\n",
    "            s1_tgt_path = cache_dir / f\"stage1_target_{idx:03d}.npy\"\n",
    "            s2_mix_path = cache_dir / f\"stage2_mixture_{idx:03d}.npy\"\n",
    "            s2_tgt_path = cache_dir / f\"stage2_target_{idx:03d}.npy\"\n",
    "            \n",
    "            np.save(s1_mix_path, mixture_s1.astype(np.float32))\n",
    "            np.save(s1_tgt_path, other.astype(np.float32))\n",
    "            np.save(s2_mix_path, mixture_s2.astype(np.float32))\n",
    "            np.save(s2_tgt_path, other.astype(np.float32))\n",
    "            \n",
    "            stage1_mixture_paths.append(str(s1_mix_path))\n",
    "            stage1_target_paths.append(str(s1_tgt_path))\n",
    "            stage2_mixture_paths.append(str(s2_mix_path))\n",
    "            stage2_target_paths.append(str(s2_tgt_path))\n",
    "            \n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(tracks)} tracks...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Skipping track {idx} ({track.name}): {str(e)[:50]}\")\n",
    "            continue\n",
    "    \n",
    "    if not stage1_mixture_paths:\n",
    "        raise ValueError(\"No tracks could be processed from MUSDB18\")\n",
    "    \n",
    "    return (stage1_mixture_paths, stage1_target_paths,\n",
    "            stage2_mixture_paths, stage2_target_paths)\n",
    "\n",
    "# Prepare data from MUSDB18\n",
    "s1_mix, s1_tgt, s2_mix, s2_tgt = prepare_curriculum_data(num_tracks=50)\n",
    "\n",
    "print(f\"\\n‚úì Stage 1 (Vocals + Other ‚Üí Other): {len(s1_mix)} samples\")\n",
    "print(f\"‚úì Stage 2 (Full Mixture ‚Üí Other): {len(s2_mix)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for both stages\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING DATALOADERS FOR CURRICULUM LEARNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "stft_processor = STFTProcessor(n_fft=2048, hop_length=512)\n",
    "\n",
    "# Stage 1: Vocals extraction\n",
    "print(\"\\nStage 1: Vocals Extraction\")\n",
    "stage1_dataset = SourceSeparationDataset(\n",
    "    mixture_paths=s1_mix,\n",
    "    target_paths=s1_tgt,\n",
    "    stft_processor=stft_processor,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "s1_train_size = int(0.8 * len(stage1_dataset))\n",
    "s1_val_size = len(stage1_dataset) - s1_train_size\n",
    "s1_train_data, s1_val_data = random_split(stage1_dataset, [s1_train_size, s1_val_size])\n",
    "\n",
    "s1_train_loader = DataLoader(s1_train_data, batch_size=4, shuffle=True, num_workers=0)\n",
    "s1_val_loader = DataLoader(s1_val_data, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"  Train: {len(s1_train_data)} | Val: {len(s1_val_data)}\")\n",
    "\n",
    "# Stage 2: Other (piano) extraction\n",
    "print(\"\\nStage 2: Other/Piano Extraction\")\n",
    "stage2_dataset = SourceSeparationDataset(\n",
    "    mixture_paths=s2_mix,\n",
    "    target_paths=s2_tgt,\n",
    "    stft_processor=stft_processor,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "s2_train_size = int(0.8 * len(stage2_dataset))\n",
    "s2_val_size = len(stage2_dataset) - s2_train_size\n",
    "s2_train_data, s2_val_data = random_split(stage2_dataset, [s2_train_size, s2_val_size])\n",
    "\n",
    "s2_train_loader = DataLoader(s2_train_data, batch_size=4, shuffle=True, num_workers=0)\n",
    "s2_val_loader = DataLoader(s2_val_data, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"  Train: {len(s2_train_data)} | Val: {len(s2_val_data)}\")\n",
    "\n",
    "print(\"\\n‚úì All dataloaders created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and trainer\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL INITIALIZATION & CHECKPOINT MANAGEMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_config = {\n",
    "    'in_channels': 1,\n",
    "    'base_channels': 32,\n",
    "    'depth': 4,\n",
    "    'use_batch_norm': True\n",
    "}\n",
    "\n",
    "model = FrequencyDomainUNet(**model_config).to(device)\n",
    "print(f\"\\n‚úì Model created: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Checkpoint management\n",
    "def train_stage(stage_num, train_loader, val_loader, num_epochs=20):\n",
    "    \"\"\"Train a curriculum stage with checkpoint management\"\"\"\n",
    "    \n",
    "    checkpoint_path = checkpoints_dir / f\"stage{stage_num}_modelA.pt\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"STAGE {stage_num}: CURRICULUM LEARNING\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Check if checkpoint exists\n",
    "    if checkpoint_path.exists():\n",
    "        print(f\"‚úì Checkpoint weights loaded: {checkpoint_path.name}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"  Epoch: {checkpoint.get('epoch', '?')} | Val Loss: {checkpoint.get('val_loss', '?'):.6f}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ModelATrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        learning_rate=1e-3,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        trainer.optimizer,\n",
    "        step_size=5,\n",
    "        gamma=0.5\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Starting training... (no checkpoint found)\")\n",
    "    history = trainer.train(num_epochs=num_epochs, save_dir=str(checkpoints_dir))\n",
    "    \n",
    "    # Save checkpoint\n",
    "    best_epoch = np.argmin(history['val_loss']) + 1\n",
    "    torch.save({\n",
    "        'epoch': best_epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'val_loss': float(np.min(history['val_loss'])),\n",
    "        'train_loss': [float(x) for x in history['train_loss']],\n",
    "        'val_loss_history': [float(x) for x in history['val_loss']]\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"\\n‚úì Checkpoint saved: {checkpoint_path.name}\")\n",
    "    print(f\"  Best epoch: {best_epoch} | Val Loss: {np.min(history['val_loss']):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Stage 1: Vocals Extraction\n",
    "train_stage(\n",
    "    stage_num=1,\n",
    "    train_loader=s1_train_loader,\n",
    "    val_loader=s1_val_loader,\n",
    "    num_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e64027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Stage 1 Performance\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STAGE 1 EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load Stage 1 checkpoint\n",
    "stage1_checkpoint = checkpoints_dir / 'stage1_modelA.pt'\n",
    "if stage1_checkpoint.exists():\n",
    "    checkpoint = torch.load(stage1_checkpoint, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"\\n‚úì Loaded Stage 1 checkpoint\")\n",
    "    print(f\"  Training epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  Validation loss: {checkpoint['val_loss']:.6f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss_total = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_data in s1_val_loader:\n",
    "            # Extract mixture magnitude and target magnitude from batch\n",
    "            mixture = batch_data['mixture_mag'].to(device)\n",
    "            target = batch_data['target_mag'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(mixture)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_fn = torch.nn.L1Loss()\n",
    "            loss = loss_fn(output, target)\n",
    "            val_loss_total += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_val_loss = val_loss_total / num_batches\n",
    "    print(f\"\\n‚úì Average validation loss: {avg_val_loss:.6f}\")\n",
    "    \n",
    "    # Test on a sample\n",
    "    print(\"\\nTesting on sample audio...\")\n",
    "    test_idx = 0\n",
    "    test_mix = np.load(s1_mix[test_idx])\n",
    "    test_tgt = np.load(s1_tgt[test_idx])\n",
    "    \n",
    "    # Create inference engine\n",
    "    inference_engine = ModelAInference(\n",
    "        model=model,\n",
    "        stft_processor=stft_processor,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Separate\n",
    "    separated = inference_engine.separate(test_mix)\n",
    "    \n",
    "    # Compute metrics\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    mse = mean_squared_error(test_tgt, separated)\n",
    "    mae = mean_absolute_error(test_tgt, separated)\n",
    "    \n",
    "    print(f\"  MSE: {mse:.6f}\")\n",
    "    print(f\"  MAE: {mae:.6f}\")\n",
    "    \n",
    "    # Audio playback\n",
    "    print(\"\\nüìä Listen to Stage 1 results:\")\n",
    "    sr = 22050\n",
    "    \n",
    "    def norm_audio(x):\n",
    "        return x / (np.max(np.abs(x)) + 1e-8) * 0.95\n",
    "    \n",
    "    print(\"\\n1. Input (Vocals + Other):\")\n",
    "    display(Audio(norm_audio(test_mix), rate=sr))\n",
    "    \n",
    "    print(\"\\n2. Target (Other/Piano):\")\n",
    "    display(Audio(norm_audio(test_tgt), rate=sr))\n",
    "    \n",
    "    print(\"\\n3. Separated (Stage 1 Output):\")\n",
    "    display(Audio(norm_audio(separated), rate=sr))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Stage 1 evaluation complete. Ready for Stage 2 training.\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Stage 1 checkpoint not found. Please run Stage 1 training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b56a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Stage 2: Other/Piano Extraction\n",
    "train_stage(\n",
    "    stage_num=2,\n",
    "    train_loader=s2_train_loader,\n",
    "    val_loader=s2_val_loader,\n",
    "    num_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model (Stage 2)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING TRAINED MODEL FOR INFERENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_checkpoint = checkpoints_dir / 'stage2_modelA.pt'\n",
    "if best_checkpoint.exists():\n",
    "    checkpoint = torch.load(best_checkpoint, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"\\n‚úì Loaded: {best_checkpoint.name}\")\n",
    "    print(f\"  Best epoch: {checkpoint['epoch']} | Val Loss: {checkpoint['val_loss']:.6f}\")\n",
    "else:\n",
    "    print(\"‚úì Using current model (no checkpoint)\")\n",
    "\n",
    "inference_engine = ModelAInference(\n",
    "    model=model,\n",
    "    stft_processor=stft_processor,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"‚úì Inference engine ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on uploaded song & database samples - with STFT visualization\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "# Clear matplotlib cache BEFORE importing\n",
    "import os\n",
    "import shutil\n",
    "cache_dir = os.path.expanduser('~/.matplotlib')\n",
    "if os.path.exists(cache_dir):\n",
    "    try:\n",
    "        shutil.rmtree(cache_dir)\n",
    "        print(\"‚úì Cleared matplotlib cache\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# NOW import matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Force matplotlib to rebuild font cache\n",
    "try:\n",
    "    matplotlib.font_manager._rebuild()\n",
    "    print(\"‚úì Rebuilt font cache\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Use simple, safe backend and minimal text\n",
    "matplotlib.use('agg')\n",
    "matplotlib.rcParams.update({\n",
    "    'font.size': 9,\n",
    "    'font.family': 'sans-serif',\n",
    "    'figure.dpi': 80,\n",
    "    'savefig.dpi': 80,\n",
    "    'text.usetex': False,\n",
    "    'axes.unicode_minus': False\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING ON UPLOADED SONG & DATABASE SAMPLES WITH STFT VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def visualize_stft_masking(mixture, separated, stft_processor, sr, title_prefix=\"\"):\n",
    "    \"\"\"Visualize STFT magnitude before and after masking with titles\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Compute STFT\n",
    "        mix_mag, mix_phase = stft_processor.waveform_to_magnitude_phase(mixture)\n",
    "        sep_mag, sep_phase = stft_processor.waveform_to_magnitude_phase(separated)\n",
    "        \n",
    "        # Create simple figure WITH titles\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4), dpi=80)\n",
    "        \n",
    "        # 1. Mixture\n",
    "        mix_db = 20 * np.log10(mix_mag + 1e-8)\n",
    "        im1 = axes[0].imshow(mix_db, aspect='auto', origin='lower', cmap='viridis')\n",
    "        axes[0].set_title(f'{title_prefix}Mixture', fontsize=10, pad=5)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # 2. Separated\n",
    "        sep_db = 20 * np.log10(sep_mag + 1e-8)\n",
    "        im2 = axes[1].imshow(sep_db, aspect='auto', origin='lower', cmap='viridis')\n",
    "        axes[1].set_title(f'{title_prefix}Separated', fontsize=10, pad=5)\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # 3. Mask (purple-orange plasma)\n",
    "        mask = sep_mag / (mix_mag + 1e-8)\n",
    "        mask = np.clip(mask, 0, 1)\n",
    "        im3 = axes[2].imshow(mask, aspect='auto', origin='lower', cmap='plasma', vmin=0, vmax=1)\n",
    "        axes[2].set_title(f'{title_prefix}Mask', fontsize=10, pad=5)\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.subplots_adjust(left=0.02, right=0.98, top=0.88, bottom=0.02, wspace=0.05)\n",
    "        \n",
    "        # Display directly\n",
    "        from IPython.display import display as ipy_display\n",
    "        ipy_display(fig)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        print(f\"‚úì Spectrograms: Mixture (blue-green) | Separated (blue-green) | Mask (purple-orange)\")\n",
    "        \n",
    "        return mix_mag, sep_mag, mask\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Visualization failed: {e}\")\n",
    "        plt.close('all')\n",
    "        return None, None, None\n",
    "\n",
    "def norm_audio(x):\n",
    "    \"\"\"Normalize audio for playback\"\"\"\n",
    "    return x / (np.max(np.abs(x)) + 1e-8) * 0.95\n",
    "\n",
    "def process_long_audio(audio_path, inference_engine, max_chunk_duration=30, sr=22050):\n",
    "    \"\"\"Process long audio files in chunks to avoid memory issues\"\"\"\n",
    "    print(f\"\\n‚ö†Ô∏è Long audio detected. Processing in chunks ({max_chunk_duration}s each)...\")\n",
    "    \n",
    "    duration = librosa.get_duration(filename=str(audio_path))\n",
    "    print(f\"Total duration: {duration:.1f}s\")\n",
    "    \n",
    "    separated_chunks = []\n",
    "    num_chunks = int(np.ceil(duration / max_chunk_duration))\n",
    "    \n",
    "    for chunk_idx in range(num_chunks):\n",
    "        offset = chunk_idx * max_chunk_duration\n",
    "        y_chunk, _ = librosa.load(str(audio_path), sr=sr, mono=True, offset=offset, duration=max_chunk_duration)\n",
    "        y_chunk = y_chunk / (np.max(np.abs(y_chunk)) + 1e-8)\n",
    "        \n",
    "        print(f\"  Processing chunk {chunk_idx + 1}/{num_chunks} ({offset:.0f}s - {offset + max_chunk_duration:.0f}s)...\")\n",
    "        separated_chunk = inference_engine.separate(y_chunk)\n",
    "        separated_chunks.append(separated_chunk)\n",
    "        \n",
    "        del y_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    separated = np.concatenate(separated_chunks)\n",
    "    print(\"‚úì Chunked processing complete\")\n",
    "    return separated\n",
    "\n",
    "# ============================================================================\n",
    "# Test 1: Uploaded Song\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"TEST 1: UPLOADED SONG\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "audio_files = []\n",
    "search_dirs = [project_root / \"data\", project_root, Path(\".\")]\n",
    "for search_dir in search_dirs:\n",
    "    if search_dir.exists():\n",
    "        for ext in ['*.mp3', '*.wav', '*.flac', '*.m4a', '*.ogg']:\n",
    "            audio_files.extend(glob.glob(str(search_dir / '**' / ext), recursive=True))\n",
    "\n",
    "if audio_files:\n",
    "    test_audio_path = audio_files[0]\n",
    "    print(f\"\\n‚úì Found audio: {Path(test_audio_path).name}\")\n",
    "    \n",
    "    file_size_mb = Path(test_audio_path).stat().st_size / (1024 * 1024)\n",
    "    duration = librosa.get_duration(filename=str(test_audio_path))\n",
    "    print(f\"File size: {file_size_mb:.1f}MB | Duration: {duration:.1f}s\")\n",
    "    \n",
    "    if file_size_mb > 30 or duration > 120:\n",
    "        print(\"‚Üí Using chunked processing (memory-efficient)\")\n",
    "        separated = process_long_audio(test_audio_path, inference_engine, max_chunk_duration=30, sr=22050)\n",
    "        y, sr = librosa.load(test_audio_path, sr=22050, mono=True, duration=30)\n",
    "        test_segment = y\n",
    "    else:\n",
    "        y, sr = librosa.load(test_audio_path, sr=22050, mono=True)\n",
    "        test_segment = y\n",
    "        print(\"‚Üí Processing full audio\")\n",
    "        test_segment = test_segment / (np.max(np.abs(test_segment)) + 1e-8)\n",
    "        \n",
    "        print(\"\\nRunning source separation...\")\n",
    "        separated = inference_engine.separate(test_segment)\n",
    "    \n",
    "    duration_sec = len(test_segment) / sr\n",
    "    print(f\"‚úì Processing song ({duration_sec:.1f}s)\")\n",
    "    \n",
    "    mix_norm = norm_audio(test_segment)\n",
    "    sep_norm = norm_audio(separated[:len(test_segment)])\n",
    "    \n",
    "    print(\"\\nüìä STFT Visualization - Uploaded Song (First 30s):\")\n",
    "    visualize_stft_masking(test_segment, separated[:len(test_segment)], stft_processor, sr, \"Song - \")\n",
    "    \n",
    "    print(\"\\nüìä ORIGINAL MIXTURE (First 30s):\")\n",
    "    display(Audio(mix_norm, rate=sr))\n",
    "    \n",
    "    print(\"\\n‚ú® SEPARATED SOURCE (First 30s):\")\n",
    "    display(Audio(sep_norm, rate=sr))\n",
    "    \n",
    "    del y, test_segment, separated\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No audio files found in data/ or current directory\")\n",
    "\n",
    "# ============================================================================\n",
    "# Test 2: Database Sample (from curriculum cache)\n",
    "# ============================================================================\n",
    "print(\"\\n\\n\" + \"-\"*70)\n",
    "print(\"TEST 2: DATABASE SAMPLE (FROM CURRICULUM CACHE)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "if 's2_mix' in locals() and s2_mix and s2_tgt:\n",
    "    sample_idx = np.random.randint(0, min(10, len(s2_mix)))\n",
    "    \n",
    "    db_mixture_path = s2_mix[sample_idx]\n",
    "    db_target_path = s2_tgt[sample_idx]\n",
    "    \n",
    "    print(f\"\\n‚úì Selected sample: {Path(db_mixture_path).name}\")\n",
    "    \n",
    "    db_mixture = np.load(db_mixture_path).astype(np.float32)\n",
    "    db_target = np.load(db_target_path).astype(np.float32)\n",
    "    \n",
    "    print(f\"‚úì Sample duration: {len(db_mixture) / 22050:.2f}s\")\n",
    "    \n",
    "    db_mixture = db_mixture / (np.max(np.abs(db_mixture)) + 1e-8)\n",
    "    db_target = db_target / (np.max(np.abs(db_target)) + 1e-8)\n",
    "    \n",
    "    print(\"\\nRunning source separation...\")\n",
    "    db_separated = inference_engine.separate(db_mixture)\n",
    "    \n",
    "    mix_norm_db = norm_audio(db_mixture)\n",
    "    tgt_norm_db = norm_audio(db_target)\n",
    "    sep_norm_db = norm_audio(db_separated)\n",
    "    \n",
    "    print(\"\\nüìä STFT Visualization - Database Sample:\")\n",
    "    visualize_stft_masking(db_mixture, db_separated, stft_processor, sr=22050, title_prefix=\"DB - \")\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    db_mse = mean_squared_error(db_target, db_separated)\n",
    "    db_mae = mean_absolute_error(db_target, db_separated)\n",
    "    \n",
    "    print(f\"\\nüìà Performance Metrics:\")\n",
    "    print(f\"  MSE: {db_mse:.6f}\")\n",
    "    print(f\"  MAE: {db_mae:.6f}\")\n",
    "    \n",
    "    print(\"\\nüìä INPUT MIXTURE (Full 4-source):\")\n",
    "    display(Audio(mix_norm_db, rate=22050))\n",
    "    \n",
    "    print(\"\\n‚úì GROUND TRUTH TARGET (Other/Piano):\")\n",
    "    display(Audio(tgt_norm_db, rate=22050))\n",
    "    \n",
    "    print(\"\\n‚ú® MODEL OUTPUT (Separated):\")\n",
    "    display(Audio(sep_norm_db, rate=22050))\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No database samples available. Please run curriculum data preparation first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
