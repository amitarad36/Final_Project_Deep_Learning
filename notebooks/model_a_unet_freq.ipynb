{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10b72ae",
   "metadata": {},
   "source": [
    "# Model A: Curriculum Learning - Other/Piano Extraction\n",
    "\n",
    "**Curriculum Learning Strategy:**\n",
    "1. **Stage 1:** Extract \"other\" from simplified mixture (vocals + other only)\n",
    "2. **Stage 2:** Extract \"other\" from full mixture (drums + bass + vocals + other)\n",
    "\n",
    "**MUSDB18 Dataset:** 4 stems per track (drums, bass, other, vocals)\n",
    "\n",
    "**Workflow:**\n",
    "- Load MUSDB18 ‚Üí prepare curriculum batches\n",
    "- Train Stage 1 on simpler 2-source task\n",
    "- Train Stage 2 on full 4-source mixture using Stage 1 weights\n",
    "- Test on uploaded song (10 seconds from 1:00-1:10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c340beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete | Device: cpu | Project: C:\\Users\\amita\\source\\repos\\Deep learning on computational accelerators\\Final_Project_Deep_Learning\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Setup paths\n",
    "project_root = Path(os.getcwd()).resolve()\n",
    "if project_root.name.lower() == \"notebooks\":\n",
    "    project_root = project_root.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "checkpoints_dir = project_root / \"checkpoints\"\n",
    "data_dir = project_root / \"data\"\n",
    "\n",
    "checkpoints_dir.mkdir(exist_ok=True, parents=True)\n",
    "data_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"‚úì Setup complete | Device: {device} | Project: {project_root}\")\n",
    "\n",
    "# Import model components\n",
    "from models.model_a_unet_freq import (\n",
    "    STFTProcessor, FrequencyDomainUNet, \n",
    "    SourceSeparationDataset, ModelATrainer, ModelAInference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767ac252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING MUSDB18 DATASET\n",
      "======================================================================\n",
      "‚úì musdb library found\n",
      "\n",
      "Loading MUSDB18 (auto-downloading if needed)...\n",
      "Note: First run may take time. Dataset will be cached for future use.\n",
      "\n",
      "‚úì MUSDB18 loaded successfully!\n",
      "‚úì Available tracks: 144\n"
     ]
    }
   ],
   "source": [
    "# Load MUSDB18 dataset - Auto download if not present\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING MUSDB18 DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    import musdb\n",
    "    print(\"‚úì musdb library found\")\n",
    "except ImportError:\n",
    "    print(\"Installing musdb...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"musdb\", \"-q\"])\n",
    "    import musdb\n",
    "\n",
    "print(\"\\nLoading MUSDB18 (auto-downloading if needed)...\")\n",
    "print(\"Note: First run may take time. Dataset will be cached for future use.\\n\")\n",
    "\n",
    "try:\n",
    "    mus = musdb.DB(download=True)\n",
    "    tracks = mus.tracks\n",
    "    print(f\"‚úì MUSDB18 loaded successfully!\")\n",
    "    print(f\"‚úì Available tracks: {len(tracks)}\")\n",
    "    use_real_musdb = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load MUSDB18: {e}\")\n",
    "    print(\"Will use synthetic data instead\")\n",
    "    mus = None\n",
    "    use_real_musdb = False\n",
    "\n",
    "# MUSDB18 stems: [0]=drums, [1]=bass, [2]=other, [3]=vocals\n",
    "STEM_NAMES = {0: 'drums', 1: 'bass', 2: 'other', 3: 'vocals'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7a1c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MUSDB18 already available\n"
     ]
    }
   ],
   "source": [
    "# Manual MUSDB18 Download Instructions\n",
    "if not use_real_musdb:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MUSDB18 DATASET REQUIRED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nThe musdb library doesn't support automatic downloads.\")\n",
    "    print(\"Please follow these steps to download MUSDB18:\\n\")\n",
    "    print(\"1. Visit: https://sigsep.github.io/datasets/musdb.html\")\n",
    "    print(\"2. Download the MUSDB18-HQ dataset (~23GB)\")\n",
    "    print(\"3. Extract the ZIP file to:\")\n",
    "    print(f\"   {musdb_root}\")\n",
    "    print(\"\\n4. After extraction, the structure should be:\")\n",
    "    print(f\"   {musdb_root}/\")\n",
    "    print(f\"     ‚îú‚îÄ‚îÄ train/\")\n",
    "    print(f\"     ‚îÇ   ‚îú‚îÄ‚îÄ A Classic Education - NightOwl/\")\n",
    "    print(f\"     ‚îÇ   ‚îú‚îÄ‚îÄ ...\")\n",
    "    print(f\"     ‚îî‚îÄ‚îÄ test/\")\n",
    "    print(f\"         ‚îú‚îÄ‚îÄ ...\")\n",
    "    print(\"\\n5. Then re-run cells 2-3 to detect the dataset\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "else:\n",
    "    print(\"‚úì MUSDB18 already available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb748fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CURRICULUM LEARNING DATA PREPARATION\n",
      "======================================================================\n",
      "\n",
      "Processing 50 MUSDB18 tracks for curriculum learning...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (stage1_mixture_paths, stage1_target_paths,\n\u001b[32m     79\u001b[39m             stage2_mixture_paths, stage2_target_paths)\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Prepare data from MUSDB18\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m s1_mix, s1_tgt, s2_mix, s2_tgt = \u001b[43mprepare_curriculum_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_tracks\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì Stage 1 (Vocals + Other ‚Üí Other): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(s1_mix)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Stage 2 (Full Mixture ‚Üí Other): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(s2_mix)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mprepare_curriculum_data\u001b[39m\u001b[34m(num_tracks)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sr != \u001b[32m22050\u001b[39m:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m signal\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     n_samples = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m22050\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m)\n\u001b[32m     45\u001b[39m     other = signal.resample(other, n_samples)\n\u001b[32m     46\u001b[39m     mixture_s1 = signal.resample(mixture_s1, n_samples)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for /: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Prepare curriculum learning data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CURRICULUM LEARNING DATA PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def prepare_curriculum_data(num_tracks=50):\n",
    "    \"\"\"Prepare data for curriculum learning using MUSDB18\"\"\"\n",
    "    \n",
    "    if not use_real_musdb or mus is None:\n",
    "        raise ValueError(\n",
    "            \"MUSDB18 dataset could not be loaded.\\n\"\n",
    "            \"Please check your internet connection and try again.\\n\"\n",
    "            \"The musdb library will attempt to download automatically.\"\n",
    "        )\n",
    "    \n",
    "    tracks = mus.tracks[:num_tracks]\n",
    "    print(f\"\\nProcessing {len(tracks)} MUSDB18 tracks for curriculum learning...\")\n",
    "    \n",
    "    stage1_mixture_paths = []\n",
    "    stage1_target_paths = []\n",
    "    stage2_mixture_paths = []\n",
    "    stage2_target_paths = []\n",
    "    \n",
    "    cache_dir = data_dir / \"curriculum_cache\"\n",
    "    cache_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    for idx, track in enumerate(tracks):\n",
    "        try:\n",
    "            # Extract stems\n",
    "            drums = track.targets['drums'].audio\n",
    "            bass = track.targets['bass'].audio\n",
    "            other = track.targets['other'].audio\n",
    "            vocals = track.targets['vocals'].audio\n",
    "            \n",
    "            # Create mixtures\n",
    "            # Stage 1: vocals + other (simplified)\n",
    "            mixture_s1 = vocals + other\n",
    "            # Stage 2: drums + bass + other + vocals (full)\n",
    "            mixture_s2 = drums + bass + other + vocals\n",
    "            \n",
    "            # Resample to 22050 Hz if needed\n",
    "            # Use MUSDB18 default sample rate (44100 Hz)\n",
    "            sr = getattr(track, 'sample_rate', None) or 44100\n",
    "            \n",
    "            if sr != 22050:\n",
    "                from scipy import signal\n",
    "                n_samples = int(len(other) * 22050 / sr)\n",
    "                other = signal.resample(other, n_samples)\n",
    "                mixture_s1 = signal.resample(mixture_s1, n_samples)\n",
    "                mixture_s2 = signal.resample(mixture_s2, n_samples)\n",
    "            \n",
    "            # Convert stereo to mono\n",
    "            if other.ndim > 1:\n",
    "                other = np.mean(other, axis=1)\n",
    "            if mixture_s1.ndim > 1:\n",
    "                mixture_s1 = np.mean(mixture_s1, axis=1)\n",
    "            if mixture_s2.ndim > 1:\n",
    "                mixture_s2 = np.mean(mixture_s2, axis=1)\n",
    "            \n",
    "            # Normalize\n",
    "            other = other / (np.max(np.abs(other)) + 1e-8)\n",
    "            mixture_s1 = mixture_s1 / (np.max(np.abs(mixture_s1)) + 1e-8)\n",
    "            mixture_s2 = mixture_s2 / (np.max(np.abs(mixture_s2)) + 1e-8)\n",
    "            \n",
    "            # Save files\n",
    "            s1_mix_path = cache_dir / f\"stage1_mixture_{idx:03d}.npy\"\n",
    "            s1_tgt_path = cache_dir / f\"stage1_target_{idx:03d}.npy\"\n",
    "            s2_mix_path = cache_dir / f\"stage2_mixture_{idx:03d}.npy\"\n",
    "            s2_tgt_path = cache_dir / f\"stage2_target_{idx:03d}.npy\"\n",
    "            \n",
    "            np.save(s1_mix_path, mixture_s1.astype(np.float32))\n",
    "            np.save(s1_tgt_path, other.astype(np.float32))\n",
    "            np.save(s2_mix_path, mixture_s2.astype(np.float32))\n",
    "            np.save(s2_tgt_path, other.astype(np.float32))\n",
    "            \n",
    "            stage1_mixture_paths.append(str(s1_mix_path))\n",
    "            stage1_target_paths.append(str(s1_tgt_path))\n",
    "            stage2_mixture_paths.append(str(s2_mix_path))\n",
    "            stage2_target_paths.append(str(s2_tgt_path))\n",
    "            \n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(tracks)} tracks...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Skipping track {idx} ({track.name}): {str(e)[:50]}\")\n",
    "            continue\n",
    "    \n",
    "    if not stage1_mixture_paths:\n",
    "        raise ValueError(\"No tracks could be processed from MUSDB18\")\n",
    "    \n",
    "    return (stage1_mixture_paths, stage1_target_paths,\n",
    "            stage2_mixture_paths, stage2_target_paths)\n",
    "\n",
    "# Prepare data from MUSDB18\n",
    "s1_mix, s1_tgt, s2_mix, s2_tgt = prepare_curriculum_data(num_tracks=50)\n",
    "\n",
    "print(f\"\\n‚úì Stage 1 (Vocals + Other ‚Üí Other): {len(s1_mix)} samples\")\n",
    "print(f\"‚úì Stage 2 (Full Mixture ‚Üí Other): {len(s2_mix)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for both stages\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING DATALOADERS FOR CURRICULUM LEARNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "stft_processor = STFTProcessor(n_fft=2048, hop_length=512)\n",
    "\n",
    "# Stage 1: Vocals extraction\n",
    "print(\"\\nStage 1: Vocals Extraction\")\n",
    "stage1_dataset = SourceSeparationDataset(\n",
    "    mixture_paths=s1_mix,\n",
    "    target_paths=s1_tgt,\n",
    "    stft_processor=stft_processor,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "s1_train_size = int(0.8 * len(stage1_dataset))\n",
    "s1_val_size = len(stage1_dataset) - s1_train_size\n",
    "s1_train_data, s1_val_data = random_split(stage1_dataset, [s1_train_size, s1_val_size])\n",
    "\n",
    "s1_train_loader = DataLoader(s1_train_data, batch_size=4, shuffle=True, num_workers=0)\n",
    "s1_val_loader = DataLoader(s1_val_data, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"  Train: {len(s1_train_data)} | Val: {len(s1_val_data)}\")\n",
    "\n",
    "# Stage 2: Other (piano) extraction\n",
    "print(\"\\nStage 2: Other/Piano Extraction\")\n",
    "stage2_dataset = SourceSeparationDataset(\n",
    "    mixture_paths=s2_mix,\n",
    "    target_paths=s2_tgt,\n",
    "    stft_processor=stft_processor,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "s2_train_size = int(0.8 * len(stage2_dataset))\n",
    "s2_val_size = len(stage2_dataset) - s2_train_size\n",
    "s2_train_data, s2_val_data = random_split(stage2_dataset, [s2_train_size, s2_val_size])\n",
    "\n",
    "s2_train_loader = DataLoader(s2_train_data, batch_size=4, shuffle=True, num_workers=0)\n",
    "s2_val_loader = DataLoader(s2_val_data, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"  Train: {len(s2_train_data)} | Val: {len(s2_val_data)}\")\n",
    "\n",
    "print(\"\\n‚úì All dataloaders created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and trainer\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL INITIALIZATION & CHECKPOINT MANAGEMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_config = {\n",
    "    'in_channels': 1,\n",
    "    'base_channels': 32,\n",
    "    'depth': 4,\n",
    "    'use_batch_norm': True\n",
    "}\n",
    "\n",
    "model = FrequencyDomainUNet(**model_config).to(device)\n",
    "print(f\"\\n‚úì Model created: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Checkpoint management\n",
    "def train_stage(stage_num, train_loader, val_loader, num_epochs=20):\n",
    "    \"\"\"Train a curriculum stage with checkpoint management\"\"\"\n",
    "    \n",
    "    checkpoint_path = checkpoints_dir / f\"stage{stage_num}_model.pt\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"STAGE {stage_num}: CURRICULUM LEARNING\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Check if checkpoint exists\n",
    "    if checkpoint_path.exists():\n",
    "        print(f\"‚úì Checkpoint weights loaded: {checkpoint_path.name}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"  Epoch: {checkpoint.get('epoch', '?')} | Val Loss: {checkpoint.get('val_loss', '?'):.6f}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ModelATrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        learning_rate=1e-3,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        trainer.optimizer,\n",
    "        step_size=5,\n",
    "        gamma=0.5\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"Starting training... (no checkpoint found)\")\n",
    "    history = trainer.train(num_epochs=num_epochs, save_dir=str(checkpoints_dir))\n",
    "    \n",
    "    # Save checkpoint\n",
    "    best_epoch = np.argmin(history['val_loss']) + 1\n",
    "    torch.save({\n",
    "        'epoch': best_epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'val_loss': float(np.min(history['val_loss'])),\n",
    "        'train_loss': [float(x) for x in history['train_loss']],\n",
    "        'val_loss_history': [float(x) for x in history['val_loss']]\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"\\n‚úì Checkpoint saved: {checkpoint_path.name}\")\n",
    "    print(f\"  Best epoch: {best_epoch} | Val Loss: {np.min(history['val_loss']):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Stage 1: Vocals Extraction\n",
    "train_stage(\n",
    "    stage_num=1,\n",
    "    train_loader=s1_train_loader,\n",
    "    val_loader=s1_val_loader,\n",
    "    num_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e64027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Stage 1 Performance\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STAGE 1 EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load Stage 1 checkpoint\n",
    "stage1_checkpoint = checkpoints_dir / 'stage1_modelA.pt'\n",
    "if stage1_checkpoint.exists():\n",
    "    checkpoint = torch.load(stage1_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"\\n‚úì Loaded Stage 1 checkpoint\")\n",
    "    print(f\"  Training epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  Validation loss: {checkpoint['val_loss']:.6f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss_total = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (mixture, target) in enumerate(s1_val_loader):\n",
    "            mixture = mixture.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(mixture)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_fn = torch.nn.L1Loss()\n",
    "            loss = loss_fn(output, target)\n",
    "            val_loss_total += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_val_loss = val_loss_total / num_batches\n",
    "    print(f\"\\n‚úì Average validation loss: {avg_val_loss:.6f}\")\n",
    "    \n",
    "    # Test on a sample\n",
    "    print(\"\\nTesting on sample audio...\")\n",
    "    test_idx = 0\n",
    "    test_mix = np.load(s1_mix[test_idx])\n",
    "    test_tgt = np.load(s1_tgt[test_idx])\n",
    "    \n",
    "    # Create inference engine\n",
    "    inference_engine = ModelAInference(\n",
    "        model=model,\n",
    "        stft_processor=stft_processor,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Separate\n",
    "    separated = inference_engine.separate(test_mix)\n",
    "    \n",
    "    # Compute metrics\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    mse = mean_squared_error(test_tgt, separated)\n",
    "    mae = mean_absolute_error(test_tgt, separated)\n",
    "    \n",
    "    print(f\"  MSE: {mse:.6f}\")\n",
    "    print(f\"  MAE: {mae:.6f}\")\n",
    "    \n",
    "    # Audio playback\n",
    "    print(\"\\nüìä Listen to Stage 1 results:\")\n",
    "    sr = 22050\n",
    "    \n",
    "    def norm_audio(x):\n",
    "        return x / (np.max(np.abs(x)) + 1e-8) * 0.95\n",
    "    \n",
    "    print(\"\\n1. Input (Vocals + Other):\")\n",
    "    display(Audio(norm_audio(test_mix), rate=sr))\n",
    "    \n",
    "    print(\"\\n2. Target (Other/Piano):\")\n",
    "    display(Audio(norm_audio(test_tgt), rate=sr))\n",
    "    \n",
    "    print(\"\\n3. Separated (Stage 1 Output):\")\n",
    "    display(Audio(norm_audio(separated), rate=sr))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Stage 1 evaluation complete. Ready for Stage 2 training.\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Stage 1 checkpoint not found. Please run Stage 1 training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b56a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Stage 2: Other/Piano Extraction\n",
    "train_stage(\n",
    "    stage_num=2,\n",
    "    train_loader=s2_train_loader,\n",
    "    val_loader=s2_val_loader,\n",
    "    num_epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model (Stage 2)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING TRAINED MODEL FOR INFERENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_checkpoint = checkpoints_dir / 'stage2_model.pt'\n",
    "if best_checkpoint.exists():\n",
    "    checkpoint = torch.load(best_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"\\n‚úì Loaded: {best_checkpoint.name}\")\n",
    "    print(f\"  Best epoch: {checkpoint['epoch']} | Val Loss: {checkpoint['val_loss']:.6f}\")\n",
    "else:\n",
    "    print(\"‚úì Using current model (no checkpoint)\")\n",
    "\n",
    "inference_engine = ModelAInference(\n",
    "    model=model,\n",
    "    stft_processor=stft_processor,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"‚úì Inference engine ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on uploaded song (10 seconds from 1:00-1:10)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING ON UPLOADED SONG\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find audio file to test\n",
    "import glob\n",
    "audio_files = []\n",
    "for ext in ['*.mp3', '*.wav', '*.flac', '*.m4a', '*.ogg']:\n",
    "    audio_files.extend(glob.glob(str(data_dir / '**' / ext), recursive=True))\n",
    "\n",
    "if audio_files:\n",
    "    test_audio_path = audio_files[0]\n",
    "    print(f\"\\n‚úì Found audio: {Path(test_audio_path).name}\")\n",
    "    \n",
    "    # Load audio\n",
    "    y, sr = librosa.load(test_audio_path, sr=22050, mono=True)\n",
    "    \n",
    "    # Extract 10 seconds from 1:00-1:10 (60-70 seconds)\n",
    "    start_sec, end_sec = 60, 70\n",
    "    start_sample = start_sec * sr\n",
    "    end_sample = end_sec * sr\n",
    "    \n",
    "    if len(y) >= end_sample:\n",
    "        test_segment = y[start_sample:end_sample]\n",
    "        print(f\"‚úì Extracted {end_sec-start_sec}s segment (from {start_sec}s to {end_sec}s)\")\n",
    "    else:\n",
    "        # If not enough audio, use what we have\n",
    "        test_segment = y[max(0, len(y)-sr*10):]\n",
    "        print(f\"‚ö†Ô∏è Audio too short, using last {len(test_segment)/sr:.1f}s\")\n",
    "    \n",
    "    # Normalize\n",
    "    test_segment = test_segment / (np.max(np.abs(test_segment)) + 1e-8)\n",
    "    \n",
    "    # Separate\n",
    "    print(\"\\nRunning source separation...\")\n",
    "    separated = inference_engine.separate(test_segment)\n",
    "    \n",
    "    # Normalize for playback\n",
    "    def norm_audio(x):\n",
    "        return x / (np.max(np.abs(x)) + 1e-8) * 0.95\n",
    "    \n",
    "    mix_norm = norm_audio(test_segment)\n",
    "    sep_norm = norm_audio(separated)\n",
    "    \n",
    "    print(\"\\nüìä ORIGINAL MIXTURE:\")\n",
    "    display(Audio(mix_norm, rate=sr))\n",
    "    \n",
    "    print(\"\\n‚ú® SEPARATED SOURCE (Other/Piano):\")\n",
    "    display(Audio(sep_norm, rate=sr))\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No audio files found in data/ directory\")\n",
    "    print(\"\\nTo test with your song:\")\n",
    "    print(\"1. Upload your audio file to: data/\")\n",
    "    print(\"2. Supported formats: MP3, WAV, FLAC, M4A, OGG\")\n",
    "    print(\"3. Re-run this cell\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
