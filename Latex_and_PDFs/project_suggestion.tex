\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{parskip}

% Page margins
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

% Title formatting
\title{\textbf{Project Proposal: Deep Music Source Separation} \\ \large Winter 2026 Final Project}
\author{Team Proposal}
\date{}

\begin{document}

\maketitle

\section{Executive Summary}
Instead of selecting one of the three pre-defined options (Pose Estimation, Hallucinations, Inpainting), we propose an \textbf{``Alternative Project''} focused on \textbf{Music Source Separation (MSS)}.

\textbf{The Goal:} To take a mixed music track (a ``mixture'') and use Deep Learning to extract a single instrument stem (e.g., piano) from it.

\textbf{The Research Angle:} We will implement and compare distinct deep learning paradigms, specifically investigating the impact of \textbf{Time-Domain modeling} and \textbf{Attention mechanisms} on audio quality:
\begin{enumerate}
    \item \textbf{Baseline:} Frequency Domain U-Net (Spectrogram-based).
    \item \textbf{Model B1:} Time Domain Demucs with \textbf{LSTM} (Recurrent modeling).
    \item \textbf{Model B2:} Time Domain Demucs with \textbf{Self-Attention} (Transformer modeling).
\end{enumerate}

This project fulfills the course requirement to ``train a model... working with a non-trivial amount of data'' and extends learned material in a ``research context''.

\section{Methodology \& Architectures}
We will implement three variations to isolate the effects of different architectural choices.

\subsection{Approach A: Frequency Domain (Baseline)}
\begin{itemize}
    \item \textbf{Concept:} The audio is transformed into a \textbf{Magnitude Spectrogram} (via STFT) and treated as an image. The network predicts a soft mask to separate the source.
    \item \textbf{Architecture:} \textbf{2D U-Net} with standard 2D convolutions.
    \item \textbf{Limitation:} Discards phase information, leading to imperfect reconstruction. Used here as a baseline to demonstrate the superiority of time-domain methods.
\end{itemize}

\subsection{Approach B: Time Domain (Main Contribution)}
\begin{itemize}
    \item \textbf{Core Concept:} The model operates directly on the raw \textbf{1D Waveform}. This preserves phase information and allows the network to learn its own optimal frequency decomposition.
    \item \textbf{Backbone:} A \textbf{Simplified Demucs} architecture using strided 1D convolutions for the Encoder and transposed 1D convolutions for the Decoder.
\end{itemize}

We will compare two specific variations of the \textbf{Bottleneck} (the deepest layer of the network):

\subsubsection*{Model B1: Recurrent Bottleneck (LSTM)}
\begin{itemize}
    \item \textbf{Mechanism:} Uses a bi-directional \textbf{LSTM} (Long Short-Term Memory) in the bottleneck.
    \item \textbf{Hypothesis:} LSTMs excel at capturing \textbf{local continuity} and smooth transitions between notes, but may struggle with long-term dependencies (e.g., a drum beat repeating every 4 seconds).
\end{itemize}

\subsubsection*{Model B2: Attention Bottleneck (Transformer)}
\begin{itemize}
    \item \textbf{Mechanism:} Replaces the LSTM with a \textbf{Multi-Head Self-Attention} layer (Transformer Encoder) and positional embeddings.
    \item \textbf{Hypothesis:} Attention provides a \textbf{global receptive field}, allowing the model to attend to similar sounds across the entire input window. We expect this to improve performance on repetitive structures (rhythm) but potentially require more data to converge than the LSTM.
\end{itemize}

\section{Dataset \& Training Strategy}
We will use \textbf{Supervised Learning}, creating perfect ``Ground Truth'' data efficiently.

\subsection{Dataset: MUSDB18}
This is the standard academic dataset for Music Source Separation, containing $\sim$150 tracks with isolated stems (Vocals, Drums, Bass, Other). This allows us to construct perfect ground-truth targets ($Y$) and inputs ($X$).

\subsection{Data Augmentation \& Curriculum Strategy}
To improve generalization and robustness, we will implement a dynamic data augmentation pipeline:

\begin{itemize}
    \item \textbf{Random Mixing (Remixing):} We will generate synthetic mixtures on-the-fly by summing stems from \textit{different} tracks (e.g., combining ``Drums from Song A'' with ``Piano from Song B'').
    \item \textbf{Curriculum Learning:} We will experiment with a progressive training difficulty schedule:
    \begin{itemize}
        \item \textbf{Stage 1 (Simple):} Train on mixtures of 2 sources (Target + 1 Distractor).
        \item \textbf{Stage 2 (Intermediate):} Increase to 3 sources.
        \item \textbf{Stage 3 (Full):} Train on the full mixture (Target + All Distractors).
    \end{itemize}
\end{itemize}

\section{Experiments \& Comparative Analysis}
The core of our report will focus on comparing the three models (Baseline, B1, B2).

\subsection{Quantitative Metrics}
We will evaluate all models on the held-out test set using the standard \texttt{museval} library:
\begin{itemize}
    \item \textbf{SDR (Signal-to-Distortion Ratio):} The primary metric for source separation quality.
    \item \textbf{SIR (Signal-to-Interference Ratio):} Measures how much of the "other" instruments leaked into the prediction.
    \item \textbf{SAR (Signal-to-Artifacts Ratio):} Measures "robotic" or "metallic" noise introduced by the model.
\end{itemize}

\subsection{Qualitative Analysis (The "Listen" Test)}
We will specifically analyze:
\begin{itemize}
    \item \textbf{Phase Coherence:} Does Model B (Time Domain) produce crisper audio than Model A (Spectrogram)?
    \item \textbf{Long-Range Structure:} Does Model B2 (Attention) handle repetitive drum beats better than Model B1 (LSTM)?
    \item \textbf{Transient Response:} Which model better preserves the sharp "attack" of a piano key press?
\end{itemize}

\section{Work Plan}
\begin{enumerate}
    \item \textbf{Week 1:} Setup data loader (MUSDB18) and implement the Baseline (U-Net).
    \item \textbf{Week 2:} Implement Model B1 (Demucs + LSTM) and verify waveform reconstruction.
    \item \textbf{Week 3:} Implement Model B2 (Demucs + Attention) and run training comparisons.
    \item \textbf{Week 4:} Evaluation, metric calculation, and report writing.
\end{enumerate}

\section{Feasibility & Logistics}
\begin{itemize}
    \item \textbf{Submission Deadline:} 5.3.2026.
    \item \textbf{Libraries:} PyTorch (core), \texttt{musdb} (data loading), \texttt{torchaudio}.
    \item \textbf{Compute Strategy:} We will downsample audio to 16kHz or 22kHz to manage memory usage for the Attention mechanism.
\end{itemize}

\end{document}